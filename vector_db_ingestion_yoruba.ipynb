{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database Ingestion for Yoruba RAG System\n",
    "\n",
    "## Objective\n",
    "To transform preprocessed Yoruba textual data into vector\n",
    "representations and store them in a vector database to support\n",
    "efficient semantic retrieval for the Retrieval-Augmented Generation\n",
    "(RAG) system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:15:09.168770Z",
     "iopub.status.busy": "2026-01-04T17:15:09.168406Z",
     "iopub.status.idle": "2026-01-04T17:15:51.426979Z",
     "shell.execute_reply": "2026-01-04T17:15:51.426217Z",
     "shell.execute_reply.started": "2026-01-04T17:15:09.168748Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "weaviate-client 4.19.2 requires protobuf<7.0.0,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.52.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.33.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.6.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.33.2 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.2 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-embeddings-gemini in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
      "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-gemini) (0.8.5)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-gemini) (0.14.12)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.34.1)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.173.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.40.3)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (6.33.2)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.12.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.26.1)\n",
      "Collecting protobuf (from google-generativeai>=0.5.2->llama-index-embeddings-gemini)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.12.13)\n",
      "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2025.5.1)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.11.6)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (4.3.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.20.1)\n",
      "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.1.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.70.0)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.9.1)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.4.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.26.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.16.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2.4.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.49.0rc1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (3.0.9)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (25.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.3.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-gemini) (2024.2.0)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.2\n",
      "    Uninstalling protobuf-6.33.2:\n",
      "      Successfully uninstalled protobuf-6.33.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "weaviate-client 4.19.2 requires protobuf<7.0.0,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere --quiet\n",
    "!pip install -U llama-index llama-index-llms-cohere --quiet\n",
    "!pip install llama-index-llms-gemini --quiet\n",
    "!pip install bert-score\n",
    "!pip install -U weaviate-client --quiet\n",
    "!pip install -U llama-index --quiet\n",
    "%pip install -U llama-index-embeddings-huggingface --quieta\n",
    "!pip install -U llama-index-vector-stores-weaviate --quiet\n",
    "!pip install -U bitsandbytes --quiet\n",
    "!pip install -U llama-index-llms-huggingface --quiet\n",
    "%pip install llama-index-retrievers-bm25 --quiet\n",
    "!pip install llama-index-embeddings-gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:15:51.428772Z",
     "iopub.status.busy": "2026-01-04T17:15:51.428549Z",
     "iopub.status.idle": "2026-01-04T17:15:56.114124Z",
     "shell.execute_reply": "2026-01-04T17:15:56.113220Z",
     "shell.execute_reply.started": "2026-01-04T17:15:51.428751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.33.2 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.2 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade protobuf>=4.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:15:56.115511Z",
     "iopub.status.busy": "2026-01-04T17:15:56.115271Z",
     "iopub.status.idle": "2026-01-04T17:15:57.485351Z",
     "shell.execute_reply": "2026-01-04T17:15:57.484733Z",
     "shell.execute_reply.started": "2026-01-04T17:15:56.115486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Best practice: store your credentials in environment variables\n",
    "weaviate_url = \"puifkdlvt8kgh7ga2rtuca.c0.us-west3.gcp.weaviate.cloud\"\n",
    "weaviate_api_key = \"@piKey\"\n",
    "\n",
    "\n",
    "\n",
    "client =weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(weaviate_api_key),\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:15:57.487276Z",
     "iopub.status.busy": "2026-01-04T17:15:57.486975Z",
     "iopub.status.idle": "2026-01-04T17:15:57.491373Z",
     "shell.execute_reply": "2026-01-04T17:15:57.490818Z",
     "shell.execute_reply.started": "2026-01-04T17:15:57.487255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.classes.config import Property, DataType, ReferenceProperty\n",
    "from weaviate.util import generate_uuid5\n",
    "from weaviate.classes.query import QueryReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:40:44.642094Z",
     "iopub.status.busy": "2026-01-04T17:40:44.641788Z",
     "iopub.status.idle": "2026-01-04T17:40:44.736499Z",
     "shell.execute_reply": "2026-01-04T17:40:44.735842Z",
     "shell.execute_reply.started": "2026-01-04T17:40:44.642075Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# This is optional to empty your database\n",
    "result = client.collections.delete(\"Yoruba_rag\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:15:57.690397Z",
     "iopub.status.busy": "2026-01-04T17:15:57.690208Z",
     "iopub.status.idle": "2026-01-04T17:16:02.406277Z",
     "shell.execute_reply": "2026-01-04T17:16:02.405702Z",
     "shell.execute_reply.started": "2026-01-04T17:15:57.690381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "\n",
    "from typing import Any, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class AfriBERTaEmbedding(BaseEmbedding):\n",
    "    _model: Any = None\n",
    "    _tokenizer: Any = None\n",
    "    _device: Any = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Davlan/afro-xlmr-mini\",\n",
    "        **kwargs: Any\n",
    "    ) -> None:\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "\n",
    "        # CPU device\n",
    "        self._device = torch.device(\"cpu\")\n",
    "\n",
    "        # 2. Load tokenizer (use_fast=False to avoid tokenizer.json issues)\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=False\n",
    "        )\n",
    "\n",
    "        # 3. Load model on CPU\n",
    "        self._model = AutoModel.from_pretrained(model_name).to(self._device)\n",
    "        self._model.eval()\n",
    "\n",
    "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "        )\n",
    "\n",
    "    def _embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Core embedding logic\"\"\"\n",
    "        inputs = self._tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512   # important for RAG\n",
    "        ).to(self._device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "\n",
    "            # Pool\n",
    "            embeddings = self._mean_pooling(\n",
    "                outputs.last_hidden_state,\n",
    "                inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # Normalize\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    # --- LlamaIndex required methods ---\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._embed([query])[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._embed([text])[0]\n",
    "\n",
    "    def _get_text_embedding_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self._embed(texts)\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:16:02.407356Z",
     "iopub.status.busy": "2026-01-04T17:16:02.406969Z",
     "iopub.status.idle": "2026-01-04T17:16:10.129026Z",
     "shell.execute_reply": "2026-01-04T17:16:10.128439Z",
     "shell.execute_reply.started": "2026-01-04T17:16:02.407336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 17:16:08.045213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767546968.064601     243 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767546968.070429     243 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-mini and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your HuggingFace access token here\n",
    "login(\"hf_your_token_here\")\n",
    "\n",
    "embedder = AfriBERTaEmbedding(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:01:01.069899Z",
     "iopub.status.busy": "2026-01-04T17:01:01.069496Z",
     "iopub.status.idle": "2026-01-04T17:01:01.076583Z",
     "shell.execute_reply": "2026-01-04T17:01:01.075949Z",
     "shell.execute_reply.started": "2026-01-04T17:01:01.069867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AfriBERTaEmbedding(model_name='Davlan/afro-xlmr-mini', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7e463dd17910>, num_workers=None, embeddings_cache=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:44:41.309043Z",
     "iopub.status.busy": "2025-12-03T17:44:41.308751Z",
     "iopub.status.idle": "2025-12-03T17:44:41.621196Z",
     "shell.execute_reply": "2025-12-03T17:44:41.620641Z",
     "shell.execute_reply.started": "2025-12-03T17:44:41.309020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:05:24.001871Z",
     "iopub.status.busy": "2026-01-04T17:05:24.001403Z",
     "iopub.status.idle": "2026-01-04T17:05:43.842737Z",
     "shell.execute_reply": "2026-01-04T17:05:43.841817Z",
     "shell.execute_reply.started": "2026-01-04T17:05:24.001843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 11CvPqZp2LGU7SUR_N-EGR2to6SnQ2v9N .ipynb_checkpoints\n",
      "Processing file 1FBqoA1_SnAUTQrhq-TZcX9naCPqd29Lm bbc_yoruba_news.csv\n",
      "Processing file 1IHbqXbi91jWTrZRH7srQHSNOnQk81iLP yankari_cleaned.csv\n",
      "Processing file 1MMlkLQzCZfAaayzFBjAMGi92VYygUHXF yankari.ipynb\n",
      "Processing file 1Qq0ZLb96ykNn7TmqPVEjpKnHGfrLfZLL yoruba_books_mistral.csv\n",
      "Processing file 1L8YzloHLmvo-eZYVceYy6IBJ2UU7usvl yoruba_text_domains.csv\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FBqoA1_SnAUTQrhq-TZcX9naCPqd29Lm\n",
      "To: /kaggle/working/RAG document/bbc_yoruba_news.csv\n",
      "100%|███████████████████████████████████████| 3.56M/3.56M [00:00<00:00, 152MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1IHbqXbi91jWTrZRH7srQHSNOnQk81iLP\n",
      "To: /kaggle/working/RAG document/yankari_cleaned.csv\n",
      "100%|██████████████████████████████████████| 86.0M/86.0M [00:01<00:00, 81.2MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MMlkLQzCZfAaayzFBjAMGi92VYygUHXF\n",
      "To: /kaggle/working/RAG document/yankari.ipynb\n",
      "100%|██████████████████████████████████████| 59.9k/59.9k [00:00<00:00, 87.2MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Qq0ZLb96ykNn7TmqPVEjpKnHGfrLfZLL\n",
      "To: /kaggle/working/RAG document/yoruba_books_mistral.csv\n",
      "100%|██████████████████████████████████████| 3.03M/3.03M [00:00<00:00, 67.5MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1L8YzloHLmvo-eZYVceYy6IBJ2UU7usvl\n",
      "From (redirected): https://drive.google.com/uc?id=1L8YzloHLmvo-eZYVceYy6IBJ2UU7usvl&confirm=t&uuid=33cae4af-18a1-4e96-83d1-571784c4c415\n",
      "To: /kaggle/working/RAG document/yoruba_text_domains.csv\n",
      "100%|████████████████████████████████████████| 117M/117M [00:01<00:00, 94.2MB/s]\n",
      "Download completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/RAG document/yoruba_books_mistral.csv',\n",
       " '/kaggle/working/RAG document/yankari_cleaned.csv',\n",
       " '/kaggle/working/RAG document/yoruba_text_domains.csv',\n",
       " '/kaggle/working/RAG document/bbc_yoruba_news.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "!gdown --folder https://drive.google.com/drive/folders/1h7iKCPfQYCbR-cir2TJZE9XoMAlpSLtV?usp=sharing\n",
    "warnings.filterwarnings('ignore')\n",
    "path = \"/kaggle/working/RAG document/\"\n",
    "files = os.listdir(path)\n",
    "file_paths= [path + fo for fo in files if fo.endswith('.csv')]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:06:22.866281Z",
     "iopub.status.busy": "2026-01-04T17:06:22.865651Z",
     "iopub.status.idle": "2026-01-04T17:06:28.869770Z",
     "shell.execute_reply": "2026-01-04T17:06:28.869094Z",
     "shell.execute_reply.started": "2026-01-04T17:06:22.866251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined data: (181052, 5)\n",
      "Shape of the combined data after removing duplicates: (179646, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load all CSVs into a dictionary of DataFrames\n",
    "dataframes = {path.split('/')[-1].replace('.csv', ''): pd.read_csv(path) for path in file_paths}\n",
    "\n",
    "# Access them like this:\n",
    "bbc_news_df = dataframes['bbc_yoruba_news']\n",
    "domains_df = dataframes['yoruba_text_domains']\n",
    "books_df = dataframes['yoruba_books_mistral']\n",
    "yankari_df = dataframes['yankari_cleaned']\n",
    "books_df['source'] = books_df['source'].str.replace('700267500-', '', regex=False)\n",
    "bbc_news_df.drop(columns=['title', 'date'], inplace=True)\n",
    "yankari_df.drop(columns=['type', 'source'], inplace=True)\n",
    "yankari_df.rename(columns={'text': 'content'}, inplace=True)\n",
    "bbc_news_df['domain']=bbc_news_df['domain'].str.lower()\n",
    "yankari_df['domain']=yankari_df['domain'].str.lower()\n",
    "books_df['domain']=books_df['domain'].str.lower()\n",
    "domains_df['domain']=domains_df['domain'].str.lower()\n",
    "yankari_df['domain'].replace({'fashion': 'social life'}, inplace=True)\n",
    "yankari_df['domain'].replace({'sports': 'social life'}, inplace=True)\n",
    "yankari_df['domain'].replace({'general': 'current affairs'}, inplace=True)\n",
    "books_df['domain'].replace({'books': 'culture'}, inplace=True)\n",
    "bbc_news_df['domain'].replace({'current_affairs': 'current affairs'}, inplace=True)\n",
    "bbc_news_df['domain'].replace({'social_life': 'social life'}, inplace=True)\n",
    "data=pd.concat([bbc_news_df, yankari_df, books_df, domains_df], ignore_index=True)\n",
    "print(f\"Shape of the combined data: {data.shape}\")\n",
    "data['content']=data['content'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ')\n",
    "data['content']=data['content'].str.replace('  ', ' ').str.replace('  ', ' ').str.replace('  ', ' ')\n",
    "data['content']=data['content'].str.strip()\n",
    "data.drop_duplicates(subset=['content'], inplace=True,keep='first')\n",
    "print(f\"Shape of the combined data after removing duplicates: {data.shape}\")\n",
    "data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:07:50.815953Z",
     "iopub.status.busy": "2026-01-04T17:07:50.815307Z",
     "iopub.status.idle": "2026-01-04T17:08:39.864591Z",
     "shell.execute_reply": "2026-01-04T17:08:39.863958Z",
     "shell.execute_reply.started": "2026-01-04T17:07:50.815920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# 1. Noise Removal: HTML tags, URLs, boilerplate, formatting\n",
    "def remove_noise(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    # Remove boilerplate (customize as needed)\n",
    "    text = re.sub(r'(copyright|all rights reserved|page \\d+)', ' ', text, flags=re.IGNORECASE)\n",
    "    # Remove extra formatting\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# 2. Text Normalization: lowercasing, punctuation, whitespace\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# 3. Yorùbá Unicode Normalization\n",
    "def yoruba_unicode_normalize(text):\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "\n",
    "# Full Preprocessing Pipeline\n",
    "def preprocess_pipeline(text):\n",
    "    text = remove_noise(text)\n",
    "    text = normalize_text(text)\n",
    "    text = yoruba_unicode_normalize(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data['content'] = data['content'].apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:41:38.729905Z",
     "iopub.status.busy": "2026-01-04T17:41:38.729579Z",
     "iopub.status.idle": "2026-01-04T17:41:39.113332Z",
     "shell.execute_reply": "2026-01-04T17:41:39.112781Z",
     "shell.execute_reply.started": "2026-01-04T17:41:38.729883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not client.collections.exists(\"Yoruba_rag\"):\n",
    "    client.collections.create(\n",
    "        name=\"Yoruba_rag\",\n",
    "        vectorizer_config=None,  # external embeddings\n",
    "        properties=[\n",
    "            Property(name=\"chunks_no\", data_type=DataType.TEXT),\n",
    "            Property(name=\"text\", data_type=DataType.TEXT),\n",
    "            Property(name=\"domain\", data_type=DataType.TEXT),\n",
    "            Property(name=\"source\", data_type=DataType.TEXT),\n",
    "            Property(name=\"url\", data_type=DataType.TEXT),\n",
    "            Property(name=\"chunking_strategy\", data_type=DataType.TEXT),\n",
    "            Property(name=\"original_id\", data_type=DataType.INT),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:41:46.242207Z",
     "iopub.status.busy": "2026-01-04T17:41:46.241632Z",
     "iopub.status.idle": "2026-01-04T17:41:46.246339Z",
     "shell.execute_reply": "2026-01-04T17:41:46.245632Z",
     "shell.execute_reply.started": "2026-01-04T17:41:46.242184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:41:50.766875Z",
     "iopub.status.busy": "2026-01-04T17:41:50.766598Z",
     "iopub.status.idle": "2026-01-04T17:41:50.829953Z",
     "shell.execute_reply": "2026-01-04T17:41:50.829362Z",
     "shell.execute_reply.started": "2026-01-04T17:41:50.766855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client,\n",
    "    index_name=\"Yoruba_rag\",\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:43:32.501490Z",
     "iopub.status.busy": "2026-01-04T17:43:32.500573Z",
     "iopub.status.idle": "2026-01-04T17:43:32.510435Z",
     "shell.execute_reply": "2026-01-04T17:43:32.509592Z",
     "shell.execute_reply.started": "2026-01-04T17:43:32.501458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DOMAIN CONFIG & SMART SPLITTER\n",
    "# ============================================\n",
    "DOMAIN_CONFIG = {\n",
    "    \"religion\":     {\"chunk_size\": 550, \"chunk_overlap\": 70},\n",
    "    \"culture\":      {\"chunk_size\": 500, \"chunk_overlap\": 60},\n",
    "    \"social_life\":  {\"chunk_size\": 400, \"chunk_overlap\": 50},\n",
    "    \"current_affairs\": {\"chunk_size\": 450, \"chunk_overlap\": 50},\n",
    "    \"entertainment\": {\"chunk_size\": 350, \"chunk_overlap\": 40},\n",
    "    \"default\":      {\"chunk_size\": 400, \"chunk_overlap\": 50}\n",
    "}\n",
    "\n",
    "def smart_split_text(text: str, domain: str) -> list:\n",
    "    config = DOMAIN_CONFIG.get(domain, DOMAIN_CONFIG[\"default\"])\n",
    "    chunk_size, overlap = config[\"chunk_size\"], config[\"chunk_overlap\"]\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "    if domain in [\"religion\", \"culture\"]:\n",
    "        sentences = re.split(r'(\\n\\n|\\. |\\? |! |Òwe:|Ifá sọ pé:|Báwo ni:|Ìdí ti:)', text)\n",
    "    else:\n",
    "        sentences = re.split(r'([.!?]+)', text)\n",
    "\n",
    "    reconstructed = []\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        chunk = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "        if chunk.strip():\n",
    "            reconstructed.append(chunk.strip())\n",
    "\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for sent in reconstructed:\n",
    "        if len(current) + len(sent) <= chunk_size:\n",
    "            current += \" \" + sent\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current.strip())\n",
    "            current = (current[-overlap:] + \" \" + sent) if len(current) > overlap else sent\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "\n",
    "    return chunks if chunks else [text[:chunk_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:44:36.265353Z",
     "iopub.status.busy": "2026-01-04T17:44:36.264593Z",
     "iopub.status.idle": "2026-01-04T17:44:36.350446Z",
     "shell.execute_reply": "2026-01-04T17:44:36.349636Z",
     "shell.execute_reply.started": "2026-01-04T17:44:36.265328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 179646\n",
      "Processing in 59 batches...\n"
     ]
    }
   ],
   "source": [
    "ROW_BATCH_SIZE = 3000\n",
    "EMBED_BATCH_SIZE = 128\n",
    "\n",
    "BATCH_SIZE = 3000   # rows, not chunks\n",
    "\n",
    "\n",
    "batches = np.array_split(data, max(1, len(data) // BATCH_SIZE))\n",
    "\n",
    "print(f\"Total rows to process: {len(data)}\")\n",
    "print(f\"Processing in {len(batches)} batches...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:48:47.289868Z",
     "iopub.status.busy": "2026-01-04T17:48:47.289427Z",
     "iopub.status.idle": "2026-01-04T17:48:47.295287Z",
     "shell.execute_reply": "2026-01-04T17:48:47.294714Z",
     "shell.execute_reply.started": "2026-01-04T17:48:47.289841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "CHECKPOINT_FILE = \"ingestion_checkpoint.json\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"last_batch\": -1, \"total_nodes\": 0}\n",
    "\n",
    "def save_checkpoint(batch_id, total_nodes):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"last_batch\": batch_id,\n",
    "                \"total_nodes\": total_nodes,\n",
    "                \"timestamp\": time.time()\n",
    "            },\n",
    "            f\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:49:12.979096Z",
     "iopub.status.busy": "2026-01-04T17:49:12.978423Z",
     "iopub.status.idle": "2026-01-04T17:49:12.982644Z",
     "shell.execute_reply": "2026-01-04T17:49:12.981948Z",
     "shell.execute_reply.started": "2026-01-04T17:49:12.979060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T17:50:31.169503Z",
     "iopub.status.busy": "2026-01-04T17:50:31.168873Z",
     "iopub.status.idle": "2026-01-05T00:18:10.118243Z",
     "shell.execute_reply": "2026-01-05T00:18:10.117523Z",
     "shell.execute_reply.started": "2026-01-04T17:50:31.169478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch 1/59 | Nodes: 6756 | Elapsed: 10.6 min | ETA: 617.4 min\n",
      "✅ Batch 2/59 | Nodes: 9801 | Elapsed: 17.7 min | ETA: 505.5 min\n",
      "✅ Batch 3/59 | Nodes: 12846 | Elapsed: 25.7 min | ETA: 479.8 min\n",
      "✅ Batch 4/59 | Nodes: 15891 | Elapsed: 31.1 min | ETA: 427.1 min\n",
      "✅ Batch 5/59 | Nodes: 18936 | Elapsed: 35.5 min | ETA: 383.0 min\n",
      "✅ Batch 6/59 | Nodes: 21981 | Elapsed: 43.3 min | ETA: 382.6 min\n",
      "✅ Batch 7/59 | Nodes: 25026 | Elapsed: 51.3 min | ETA: 380.7 min\n",
      "✅ Batch 8/59 | Nodes: 28071 | Elapsed: 59.4 min | ETA: 378.5 min\n",
      "✅ Batch 9/59 | Nodes: 31116 | Elapsed: 67.3 min | ETA: 373.8 min\n",
      "✅ Batch 10/59 | Nodes: 34161 | Elapsed: 74.9 min | ETA: 366.9 min\n",
      "✅ Batch 11/59 | Nodes: 37206 | Elapsed: 80.0 min | ETA: 349.1 min\n",
      "✅ Batch 12/59 | Nodes: 40251 | Elapsed: 83.1 min | ETA: 325.4 min\n",
      "✅ Batch 13/59 | Nodes: 43296 | Elapsed: 85.9 min | ETA: 304.1 min\n",
      "✅ Batch 14/59 | Nodes: 46341 | Elapsed: 88.9 min | ETA: 285.9 min\n",
      "✅ Batch 15/59 | Nodes: 49386 | Elapsed: 91.7 min | ETA: 268.9 min\n",
      "✅ Batch 16/59 | Nodes: 52431 | Elapsed: 96.5 min | ETA: 259.3 min\n",
      "✅ Batch 17/59 | Nodes: 55476 | Elapsed: 102.0 min | ETA: 251.9 min\n",
      "✅ Batch 18/59 | Nodes: 63793 | Elapsed: 112.4 min | ETA: 255.9 min\n",
      "✅ Batch 19/59 | Nodes: 72900 | Elapsed: 125.0 min | ETA: 263.2 min\n",
      "✅ Batch 20/59 | Nodes: 82073 | Elapsed: 137.6 min | ETA: 268.3 min\n",
      "✅ Batch 21/59 | Nodes: 86601 | Elapsed: 147.0 min | ETA: 266.0 min\n",
      "✅ Batch 22/59 | Nodes: 89912 | Elapsed: 150.9 min | ETA: 253.8 min\n",
      "✅ Batch 23/59 | Nodes: 92957 | Elapsed: 153.0 min | ETA: 239.4 min\n",
      "✅ Batch 24/59 | Nodes: 96002 | Elapsed: 155.0 min | ETA: 226.0 min\n",
      "✅ Batch 25/59 | Nodes: 99047 | Elapsed: 157.0 min | ETA: 213.5 min\n",
      "✅ Batch 26/59 | Nodes: 102092 | Elapsed: 159.0 min | ETA: 201.8 min\n",
      "✅ Batch 27/59 | Nodes: 105137 | Elapsed: 161.0 min | ETA: 190.8 min\n",
      "✅ Batch 28/59 | Nodes: 108182 | Elapsed: 163.0 min | ETA: 180.4 min\n",
      "✅ Batch 29/59 | Nodes: 111227 | Elapsed: 164.9 min | ETA: 170.6 min\n",
      "✅ Batch 30/59 | Nodes: 114272 | Elapsed: 166.9 min | ETA: 161.3 min\n",
      "✅ Batch 31/59 | Nodes: 117317 | Elapsed: 168.8 min | ETA: 152.5 min\n",
      "✅ Batch 32/59 | Nodes: 120362 | Elapsed: 170.8 min | ETA: 144.1 min\n",
      "✅ Batch 33/59 | Nodes: 123407 | Elapsed: 172.7 min | ETA: 136.1 min\n",
      "✅ Batch 34/59 | Nodes: 126452 | Elapsed: 174.7 min | ETA: 128.4 min\n",
      "✅ Batch 35/59 | Nodes: 129497 | Elapsed: 176.6 min | ETA: 121.1 min\n",
      "✅ Batch 37/59 | Nodes: 135587 | Elapsed: 180.5 min | ETA: 107.3 min\n",
      "✅ Batch 38/59 | Nodes: 138632 | Elapsed: 182.5 min | ETA: 100.8 min\n",
      "✅ Batch 39/59 | Nodes: 141677 | Elapsed: 184.4 min | ETA: 94.6 min\n",
      "✅ Batch 40/59 | Nodes: 144722 | Elapsed: 186.4 min | ETA: 88.5 min\n",
      "✅ Batch 41/59 | Nodes: 147767 | Elapsed: 188.3 min | ETA: 82.7 min\n",
      "✅ Batch 42/59 | Nodes: 152922 | Elapsed: 193.9 min | ETA: 78.5 min\n",
      "✅ Batch 43/59 | Nodes: 161316 | Elapsed: 204.9 min | ETA: 76.2 min\n",
      "✅ Batch 44/59 | Nodes: 165132 | Elapsed: 207.7 min | ETA: 70.8 min\n",
      "✅ Batch 45/59 | Nodes: 170837 | Elapsed: 214.3 min | ETA: 66.7 min\n",
      "✅ Batch 46/59 | Nodes: 179940 | Elapsed: 227.1 min | ETA: 64.2 min\n",
      "✅ Batch 47/59 | Nodes: 189012 | Elapsed: 240.0 min | ETA: 61.3 min\n",
      "✅ Batch 48/59 | Nodes: 198082 | Elapsed: 252.8 min | ETA: 57.9 min\n",
      "✅ Batch 49/59 | Nodes: 207153 | Elapsed: 265.6 min | ETA: 54.2 min\n",
      "✅ Batch 50/59 | Nodes: 216199 | Elapsed: 278.4 min | ETA: 50.1 min\n",
      "✅ Batch 51/59 | Nodes: 225255 | Elapsed: 291.2 min | ETA: 45.7 min\n",
      "✅ Batch 52/59 | Nodes: 234335 | Elapsed: 304.0 min | ETA: 40.9 min\n",
      "✅ Batch 53/59 | Nodes: 243384 | Elapsed: 316.8 min | ETA: 35.9 min\n",
      "✅ Batch 54/59 | Nodes: 252462 | Elapsed: 329.6 min | ETA: 30.5 min\n",
      "✅ Batch 55/59 | Nodes: 261520 | Elapsed: 342.2 min | ETA: 24.9 min\n",
      "✅ Batch 56/59 | Nodes: 270561 | Elapsed: 354.9 min | ETA: 19.0 min\n",
      "✅ Batch 57/59 | Nodes: 279610 | Elapsed: 367.6 min | ETA: 12.9 min\n",
      "✅ Batch 58/59 | Nodes: 288656 | Elapsed: 380.2 min | ETA: 6.6 min\n",
      "✅ Batch 59/59 | Nodes: 293827 | Elapsed: 387.6 min | ETA: 0.0 min\n",
      "\n",
      "🎉 INGESTION COMPLETE — 293827 nodes indexed\n"
     ]
    }
   ],
   "source": [
    "ROW_BATCH_SIZE = 3000\n",
    "EMBED_BATCH_SIZE = 128\n",
    "LOG_EVERY = 1   # batches\n",
    "\n",
    "checkpoint = load_checkpoint()\n",
    "start_batch = checkpoint[\"last_batch\"] + 1\n",
    "total_docs = checkpoint[\"total_nodes\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_id in range(start_batch, len(batches)):\n",
    "    batch = batches[batch_id]\n",
    "\n",
    "    all_chunks = []\n",
    "    chunk_meta = []\n",
    "\n",
    "    # ---- FAST ROW LOOP ----\n",
    "    for row in batch.itertuples(index=True):\n",
    "        idx = row.Index\n",
    "        text = getattr(row, \"content\", None)\n",
    "\n",
    "        if not text or not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        domain = getattr(row, \"domain\", \"default\")\n",
    "        domain_key = domain.lower().replace(\" \", \"_\")\n",
    "        if domain_key not in DOMAIN_CONFIG:\n",
    "            domain_key = \"default\"\n",
    "\n",
    "        base_meta = {\n",
    "            \"domain\": domain,\n",
    "            \"source\": getattr(row, \"source\", None),\n",
    "            \"url\": getattr(row, \"url\", None),\n",
    "            \"original_id\": int(idx),\n",
    "            \"chunking_strategy\": \"domain_aware\"\n",
    "        }\n",
    "\n",
    "        chunks = smart_split_text(text, domain_key)\n",
    "\n",
    "        for chunk_no, chunk in enumerate(chunks, start=1):\n",
    "            all_chunks.append(chunk)\n",
    "            chunk_meta.append({\n",
    "                **base_meta,\n",
    "                \"chunk_no\": chunk_no,\n",
    "                \"node_id\": str(uuid4())\n",
    "            })\n",
    "\n",
    "    # ---- EMBEDDING (BATCHED) ----\n",
    "    nodes = []\n",
    "\n",
    "    for i in range(0, len(all_chunks), EMBED_BATCH_SIZE):\n",
    "        texts = all_chunks[i:i + EMBED_BATCH_SIZE]\n",
    "        metas = chunk_meta[i:i + EMBED_BATCH_SIZE]\n",
    "\n",
    "        embeddings = embedder.get_text_embedding_batch(texts)\n",
    "\n",
    "        for emb, txt, meta in zip(embeddings, texts, metas):\n",
    "            nodes.append(\n",
    "                TextNode(\n",
    "                    id_=meta[\"node_id\"],\n",
    "                    text=txt,\n",
    "                    embedding=emb,\n",
    "                    metadata={k: v for k, v in meta.items() if k != \"node_id\"}\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # ---- WEAVIATE WRITE ----\n",
    "    if nodes:\n",
    "        vector_store.add(nodes)\n",
    "        total_docs += len(nodes)\n",
    "\n",
    "    # ---- CHECKPOINT ----\n",
    "    save_checkpoint(batch_id, total_docs)\n",
    "\n",
    "    # ---- LOGGING ----\n",
    "    elapsed = time.time() - start_time\n",
    "    batches_done = batch_id + 1\n",
    "    batches_left = len(batches) - batches_done\n",
    "    avg_time = elapsed / batches_done\n",
    "    eta = avg_time * batches_left\n",
    "\n",
    "    print(\n",
    "        f\"✅ Batch {batch_id + 1}/{len(batches)} | \"\n",
    "        f\"Nodes: {total_docs} | \"\n",
    "        f\"Elapsed: {elapsed/60:.1f} min | \"\n",
    "        f\"ETA: {eta/60:.1f} min\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n🎉 INGESTION COMPLETE — {total_docs} nodes indexed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:19:52.911944Z",
     "iopub.status.busy": "2025-12-03T17:19:52.911669Z",
     "iopub.status.idle": "2025-12-03T17:23:55.175201Z",
     "shell.execute_reply": "2025-12-03T17:23:55.174325Z",
     "shell.execute_reply.started": "2025-12-03T17:19:52.911924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 179 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing: 100%|██████████| 179/179 [04:00<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Total Documents Processed: 179646\n",
      "Total Nodes (Chunks): 402101\n",
      "Example Node ID: 0-chunk-001\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "BATCH_SIZE = 1000     # Adjust based on RAM (1000–5000 recommended)\n",
    "CHUNK_SIZE = 350\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 1. SPLIT DATAFRAME INTO BATCHES\n",
    "# ============================================\n",
    "batches = np.array_split(data, max(1, len(data) // BATCH_SIZE))\n",
    "\n",
    "all_nodes = []\n",
    "total_docs = 0\n",
    "\n",
    "# ============================================\n",
    "# 2. PROCESS EACH BATCH\n",
    "# ============================================\n",
    "print(f\"Processing {len(batches)} batches...\")\n",
    "\n",
    "for batch_id, batch in enumerate(tqdm(batches, desc=\"Batch Processing\")):\n",
    "\n",
    "    # Convert batch rows → Documents (vectorized approach)\n",
    "    documents = []\n",
    "    for idx, row in batch.iterrows():\n",
    "        content = row.get(\"content\")\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        metadata = {\n",
    "            'domain': row.get('domain'),\n",
    "            'source': row.get('source'),\n",
    "            'url': row.get('url'),\n",
    "        }\n",
    "\n",
    "        # Use original index as doc ID\n",
    "        documents.append(\n",
    "            Document(\n",
    "                text=content,\n",
    "                metadata=metadata,\n",
    "                id_=str(idx)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update total docs count\n",
    "    total_docs += len(documents)\n",
    "\n",
    "    if not documents:\n",
    "        continue\n",
    "\n",
    "    # ============================================\n",
    "    # 3. SPLIT INTO NODES WITH SENTENCE SPLITTER\n",
    "    # ============================================\n",
    "    batch_nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    # ============================================\n",
    "    # 4. FIX NODE IDS (docID-chunk-XXX)\n",
    "    # ============================================\n",
    "    doc_chunk_map = {}  # keeps chunk number for each doc\n",
    "\n",
    "    for node in batch_nodes:\n",
    "        doc_base_id = node.ref_doc_id\n",
    "\n",
    "        # auto-increment chunk_no for each doc\n",
    "        doc_chunk_map.setdefault(doc_base_id, 0)\n",
    "        doc_chunk_map[doc_base_id] += 1\n",
    "        chunk_no = doc_chunk_map[doc_base_id]\n",
    "\n",
    "        # Create nice ID\n",
    "        new_node_id = f\"{doc_base_id}-chunk-{chunk_no:03d}\"\n",
    "        node.id_ = new_node_id\n",
    "        node.metadata[\"chunks_no\"] = chunk_no\n",
    "\n",
    "    all_nodes.extend(batch_nodes)\n",
    "\n",
    "# ============================================\n",
    "# RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(\"====================================\")\n",
    "print(f\"Total Documents Processed: {total_docs}\")\n",
    "print(f\"Total Nodes (Chunks): {len(all_nodes)}\")\n",
    "if all_nodes:\n",
    "    print(f\"Example Node ID: {all_nodes[0].id_}\")\n",
    "print(\"====================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:44:52.445270Z",
     "iopub.status.busy": "2025-12-03T17:44:52.444557Z",
     "iopub.status.idle": "2025-12-03T17:44:52.449882Z",
     "shell.execute_reply": "2025-12-03T17:44:52.449227Z",
     "shell.execute_reply.started": "2025-12-03T17:44:52.445247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entertainment'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes[0].metadata.get('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:48:21.809383Z",
     "iopub.status.busy": "2025-12-03T17:48:21.809015Z",
     "iopub.status.idle": "2025-12-03T17:48:21.814534Z",
     "shell.execute_reply": "2025-12-03T17:48:21.813990Z",
     "shell.execute_reply.started": "2025-12-03T17:48:21.809359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402101"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Create collection if it doesn't exist ---\n",
    "if not client.collections.exists(\"Yoruba_rag\"):\n",
    "    collection = client.collections.create(\n",
    "        name=\"Yoruba_rag\",\n",
    "        vectorizer_config=None,  # we provide our own embeddings\n",
    "        properties=[\n",
    "            Property(name=\"chunks_no\", data_type=DataType.TEXT),\n",
    "            Property(name=\"text\", data_type=DataType.TEXT),\n",
    "            Property(name=\"domain\", data_type=DataType.TEXT),\n",
    "            Property(name=\"source\", data_type=DataType.TEXT),\n",
    "            Property(name=\"url\", data_type=DataType.TEXT),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# --- Insert documents with embeddings ---\n",
    "yoruba_collection = client.collections.get(\"Yoruba_rag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:45:05.194862Z",
     "iopub.status.busy": "2025-12-03T17:45:05.194129Z",
     "iopub.status.idle": "2025-12-03T17:45:05.770051Z",
     "shell.execute_reply": "2025-12-03T17:45:05.769377Z",
     "shell.execute_reply.started": "2025-12-03T17:45:05.194837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client reconnected.\n",
      "1\n",
      "Client connection closed.\n"
     ]
    }
   ],
   "source": [
    "client.connect() \n",
    "print(\"Client reconnected.\")\n",
    "yoruba_collection = client.collections.get(\"Yoruba_rag\")\n",
    "first_object = all_nodes[0]\n",
    "raw_properties = {\n",
    "                \"text\": first_object.text,\n",
    "                \"domain\": first_object.metadata.get('domain'),\n",
    "                \"source\": first_object.metadata.get('source'),\n",
    "                \"url\": first_object.metadata.get('url'),\n",
    "                \"chunks_no\": first_object.metadata.get('chunks_no'),\n",
    "                \"vector\" : embedder._get_text_embedding(first_object.text)\n",
    "    \n",
    "            }\n",
    "\n",
    "\n",
    "data_properties = clean_properties(raw_properties)\n",
    "\n",
    "# 👉 Correct method\n",
    "yoruba_collection.data.insert(data_properties)\n",
    "\n",
    "response = yoruba_collection.aggregate.over_all(total_count=True)\n",
    "print(response.total_count)  # Should output 1\n",
    "\n",
    "client.close()\n",
    "print(\"Client connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:55:15.388491Z",
     "iopub.status.busy": "2025-12-03T17:55:15.387714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client reconnected.\n",
      "Total items to insert: 402100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting chunks:  41%|████      | 164985/402100 [27:40<39:24, 100.30it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "client.connect()\n",
    "print(\"Client reconnected.\")\n",
    "\n",
    "yoruba_collection = client.collections.get(\"Yoruba_rag\")\n",
    "\n",
    "remaining_data = all_nodes[1:]  # your remaining nodes\n",
    "\n",
    "print(f\"Total items to insert: {len(remaining_data)}\")\n",
    "\n",
    "with yoruba_collection.batch.dynamic() as batch:\n",
    "    for item in tqdm(remaining_data, desc=\"Inserting chunks\"):\n",
    "        \n",
    "        # --- Prepare properties ---\n",
    "        raw_properties = {\n",
    "            \"text\": item.text,\n",
    "            \"domain\": item.metadata.get(\"domain\"),\n",
    "            \"source\": item.metadata.get(\"source\"),\n",
    "            \"url\": item.metadata.get(\"url\"),\n",
    "            \"chunks_no\": item.metadata.get(\"chunks_no\"),\n",
    "        }\n",
    "\n",
    "        # clean null values\n",
    "        data_properties = clean_properties(raw_properties)\n",
    "\n",
    "        # --- Compute AfriBERTa embedding ---\n",
    "        vector = embedder._get_text_embedding(item.text)\n",
    "\n",
    "        # --- Add to batch ---\n",
    "        batch.add_object(\n",
    "            properties=data_properties,\n",
    "            vector=vector\n",
    "        )\n",
    "\n",
    "# ---- Check ----\n",
    "response = yoruba_collection.aggregate.over_all(total_count=True)\n",
    "print(\"Total count in Weaviate:\", response.total_count)\n",
    "\n",
    "client.close()\n",
    "print(\"Client connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T11:36:43.492966Z",
     "iopub.status.busy": "2025-12-03T11:36:43.492037Z",
     "iopub.status.idle": "2025-12-03T11:55:20.684445Z",
     "shell.execute_reply": "2025-12-03T11:55:20.683681Z",
     "shell.execute_reply.started": "2025-12-03T11:36:43.492922Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Cleaned Batch Upload of 402101 Nodes ---\n",
      "Reconnecting Weaviate client...\n",
      "Client reconnected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning, Batching, and Inserting Nodes: 100%|██████████| 402101/402101 [18:01<00:00, 371.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dynamic Batch Upload Complete! All 402101 nodes processed.\n",
      "✅ Database Count: 0 objects in collection.\n",
      "Client connection closed.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import math\n",
    "\n",
    "# --- Your Cleaning Helpers ---\n",
    "def clean_properties(props: dict) -> dict:\n",
    "    \"\"\"Replaces None/NaN values in dictionary properties with an empty string.\"\"\"\n",
    "    clean = {}\n",
    "    for k, v in props.items():\n",
    "        if v is None:\n",
    "            clean[k] = \"\"\n",
    "        elif isinstance(v, float) and math.isnan(v): \n",
    "            clean[k] = \"\"\n",
    "        else:\n",
    "            clean[k] = str(v) \n",
    "    return clean\n",
    "\n",
    "# --- Assume client and collection are initialized and connected ---\n",
    "# client = weaviate.connect_to_wcs(...) \n",
    "COLLECTION_NAME = \"Yoruba_rag\"\n",
    "# nodes: List[TextNode] = ... \n",
    "BATCH_SIZE = 512 \n",
    "\n",
    "print(f\"\\n--- Starting Cleaned Batch Upload of {len(all_nodes)} Nodes ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Ensure the client is connected (from the last fix)\n",
    "    if not client.is_connected():\n",
    "        print(\"Reconnecting Weaviate client...\")\n",
    "        client.connect()\n",
    "        print(\"Client reconnected.\")\n",
    "\n",
    "    # 2. Get the collection handle\n",
    "    collection = client.collections.get(COLLECTION_NAME)\n",
    "\n",
    "    # 3. Start the batch operation\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        \n",
    "        for node in tqdm(all_nodes, desc=\"Cleaning, Batching, and Inserting Nodes\"):\n",
    "            \n",
    "            raw_properties = {\n",
    "                \"content\": node.text,\n",
    "                \"domain\": node.metadata.get('domain'),\n",
    "                \"source\": node.metadata.get('source'),\n",
    "                \"url\": node.metadata.get('url'),\n",
    "                \"chunks_no\": node.metadata.get('chunks_no'),\n",
    "            }\n",
    "            \n",
    "            data_properties = clean_properties(raw_properties)\n",
    "            \n",
    "            # 4. CRITICAL FIX: Use the likely correct method name: add_object()\n",
    "            batch.add_object(\n",
    "                properties=data_properties \n",
    "                # collection is inherited from the context manager\n",
    "            )\n",
    "            \n",
    "    print(f\"\\n✅ Dynamic Batch Upload Complete! All {len(all_nodes)} nodes processed.\")\n",
    "    \n",
    "    # Verification\n",
    "    count_result = collection.aggregate.over_all(total_count=True)\n",
    "    print(f\"✅ Database Count: {count_result.total_count} objects in collection.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ A critical error occurred during batch insertion: {e}\")\n",
    "finally:\n",
    "    if 'client' in locals() and client.is_connected():\n",
    "        client.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "\n",
    "from llama_index.core import Document, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.config import Property, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ✅ LlamaIndex-AfriBERTa Embedding (Mean Pooling + Normalize)\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "class AfriBERTaEmbedding(BaseEmbedding):\n",
    "    _model: AutoModel = PrivateAttr()\n",
    "    _tokenizer: AutoTokenizer = PrivateAttr()\n",
    "    _device: str = PrivateAttr()\n",
    "\n",
    "    def __init__(self, model_name: str = \"castorini/afriberta_base\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModel.from_pretrained(model_name).to(self._device)\n",
    "        self._model.eval()\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"AfriBERTaEmbedding\"\n",
    "\n",
    "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def _embed(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self._tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self._device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "            embeddings = self._mean_pooling(outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
    "            embeddings = normalize(embeddings.cpu().numpy(), norm=\"l2\")\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def _get_query_embedding(self, query: str):\n",
    "        return self._embed(query)[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str):\n",
    "        return self._embed(text)[0]\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str):\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str):\n",
    "        return self._get_text_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Create LlamaIndex embedding model (NOT LangChain!)\n",
    "embed_model = AfriBERTaEmbedding(\n",
    "    model_name=\"castorini/afriberta_base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "# Force sync CUDA errors for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"castorini/afriberta_base\"\n",
    "\n",
    "# Load tokenizer & model from SAME checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "def mean_pooling(last_hidden_state, attention_mask):\n",
    "    # average pooling of token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    return (last_hidden_state * input_mask_expanded).sum(1) / input_mask_expanded.sum(1).clamp(min=1e-9)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_text(texts, max_length=512):\n",
    "    \"\"\"Return embeddings as numpy arrays\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    outputs = model(**encoded)\n",
    "    embeddings = mean_pooling(outputs.last_hidden_state, encoded[\"attention_mask\"])\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# 🔹 Test it\n",
    "vecs = embed_text([\"Ẹ káàrọ̀, bí o ṣe wa?\", \"Hello world!\"])\n",
    "print(\"Embeddings shape:\", vecs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "embeddings_with_metadata = []\n",
    "for node in nodes:\n",
    "    text = node.get_content()\n",
    "    emb = embed_model.get_text_embedding(text)\n",
    "\n",
    "    item = {\n",
    "        \"embedding\": emb,\n",
    "        \"metadata\": {\n",
    "            \"text\": text,\n",
    "            \"domain\": node.metadata.get(\"domain\", \"\"),\n",
    "            \"source\": node.metadata.get(\"source\", \"\"),\n",
    "            \"url\": node.metadata.get(\"url\", \"\"),\n",
    "        },\n",
    "    }\n",
    "    embeddings_with_metadata.append(item)\n",
    "\n",
    "print(f\"✅ Created {len(embeddings_with_metadata)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Clean helpers\n",
    "# -------------------------------\n",
    "def clean_vector(vec):\n",
    "    arr = np.array(vec, dtype=np.float32).flatten()\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return arr.tolist()\n",
    "\n",
    "def clean_properties(props: dict) -> dict:\n",
    "    clean = {}\n",
    "    for k, v in props.items():\n",
    "        if v is None:\n",
    "            clean[k] = \"\"\n",
    "        elif isinstance(v, float) and math.isnan(v):\n",
    "            clean[k] = \"\"\n",
    "        else:\n",
    "            clean[k] = str(v)\n",
    "    return clean\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Generate embeddings + batch insert\n",
    "# -------------------------------\n",
    "with yoruba_collection.batch.dynamic() as batch:\n",
    "    for node in nodes:\n",
    "        text = node.get_content(metadata_mode=\"all\")  # node text\n",
    "        metadata = node.metadata\n",
    "\n",
    "        properties = clean_properties(metadata)\n",
    "        properties[\"text\"] = text\n",
    "\n",
    "        # Generate embedding\n",
    "        vector = clean_vector(embed_model.get_text_embedding(text))\n",
    "\n",
    "        # Add to Weaviate\n",
    "        batch.add_object(\n",
    "            properties=properties,\n",
    "            vector=vector,\n",
    "        )\n",
    "\n",
    "print(\"✅ Indexing complete (all nodes pushed to Weaviate Cloud)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client,\n",
    "    index_name=\"YorubaChunk\",\n",
    "    text_key=\"text\",\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

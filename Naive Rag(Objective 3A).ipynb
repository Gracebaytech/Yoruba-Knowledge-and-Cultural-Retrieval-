{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmEfneTk0EFH"
      },
      "outputs": [],
      "source": [
        "!pip install -U weaviate-client\n",
        "!pip install -U llama-index\n",
        "%pip install llama-index-embeddings-huggingface\n",
        "!pip install llama_index.vector_stores.weaviate\n",
        "%pip install llama-index-retrievers-bm25\n",
        "!pip install rouge\n",
        "!pip install -U transformers accelerate\n",
        "\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "from llama_index.core import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm9zaQ-N0RBi"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "import torch\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from sklearn.preprocessing import normalize\n",
        "# Embedding model2Ô∏è‚É£ AfriBERTa embedding (HuggingFace style)\n",
        "# ============================\n",
        "class AfriBERTaEmbedding(BaseEmbedding):\n",
        "    def __init__(self, model_name=\"castorini/afriberta_base\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        from transformers import AutoTokenizer, AutoModel\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self._model = AutoModel.from_pretrained(model_name).to(self._device)\n",
        "        self._model.eval()\n",
        "\n",
        "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    def _embed(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        inputs = self._tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self._device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self._model(**inputs)\n",
        "            embeddings = self._mean_pooling(outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
        "            embeddings = normalize(embeddings.cpu().numpy(), norm=\"l2\")\n",
        "        return embeddings.tolist()\n",
        "\n",
        "    # -----------------------------\n",
        "    # Implement required abstract methods\n",
        "    # -----------------------------\n",
        "    def _get_text_embedding(self, text: str):\n",
        "        return self._embed(text)[0]\n",
        "\n",
        "    def _get_query_embedding(self, query: str):\n",
        "        return self._embed(query)[0]\n",
        "\n",
        "    async def _aget_text_embedding(self, text: str):\n",
        "        return self._get_text_embedding(text)\n",
        "\n",
        "    async def _aget_query_embedding(self, query: str):\n",
        "        return self._get_query_embedding(query)\n",
        "\n",
        "from llama_index.core.settings import Settings\n",
        "Settings.embed_model = AfriBERTaEmbedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbCj-Ylc0Qh3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WEAVIATE_URL\"] = \"https://eu3nsymbtcib2x8fy120yw.c0.europe-west3.gcp.weaviate.cloud\"\n",
        "os.environ[\"WEAVIATE_API_KEY\"] = \"****************************************\"\n",
        "\n",
        "import os\n",
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "\n",
        "# Best practice: store your credentials in environment variables\n",
        "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
        "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
        "# Connect to Weaviate Cloud\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=weaviate_url,\n",
        "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
        ")\n",
        "\n",
        "print(client.is_ready())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo7-gcu90QeF"
      },
      "outputs": [],
      "source": [
        "#3. Wrap your Weaviate collection\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "vector_store = WeaviateVectorStore(\n",
        "    weaviate_client=client,\n",
        "    index_name=\"YorubaChunk\",   # use your actual collection name\n",
        "\n",
        ")\n",
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# 6Ô∏è‚É£ Create index with Qdrant\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=Settings.embed_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7sdYciq0Qa6"
      },
      "outputs": [],
      "source": [
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dense Retriever (AfriBERTa)\n",
        "dense_retriever = index.as_retriever(similarity_top_k=5)\n",
        "\n",
        "# Sparse Retriever (BM25)\n",
        "yoruba_collection = client.collections.get(\"YorubaChunk\")\n",
        "all_docs = [Document(text=obj.properties[\"text\"]) for obj in yoruba_collection.iterator()]\n",
        "sparse_retriever = BM25Retriever(all_docs, similarity_top_k=5)\n",
        "\n",
        "# Hybrid Retriever\n",
        "class HybridRetriever(BaseRetriever):\n",
        "    def __init__(self, dense_retriever, sparse_retriever, mode=\"rrf\", alpha=0.5, k=60):\n",
        "        self.dense_retriever = dense_retriever\n",
        "        self.sparse_retriever = sparse_retriever\n",
        "        self.mode = mode\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "\n",
        "    def _retrieve(self, query, **kwargs):\n",
        "        dense_results = self.dense_retriever.retrieve(query, **kwargs)\n",
        "        sparse_results = self.sparse_retriever.retrieve(query, **kwargs)\n",
        "\n",
        "        dense_dict = {r.node.node_id: (r.score, i + 1) for i, r in enumerate(dense_results)}\n",
        "        sparse_dict = {r.node.node_id: (r.score, i + 1) for i, r in enumerate(sparse_results)}\n",
        "\n",
        "        all_doc_ids = set(dense_dict.keys()) | set(sparse_dict.keys())\n",
        "        fused_scores = defaultdict(float)\n",
        "\n",
        "        if self.mode == \"rrf\":\n",
        "            for doc_id in all_doc_ids:\n",
        "                if doc_id in dense_dict:\n",
        "                    _, rank = dense_dict[doc_id]\n",
        "                    fused_scores[doc_id] += 1.0 / (self.k + rank)\n",
        "                if doc_id in sparse_dict:\n",
        "                    _, rank = sparse_dict[doc_id]\n",
        "                    fused_scores[doc_id] += 1.0 / (self.k + rank)\n",
        "        elif self.mode == \"rsf\":\n",
        "            for doc_id in all_doc_ids:\n",
        "                s_vec = dense_dict.get(doc_id, (0.0, None))[0]\n",
        "                s_bm25 = sparse_dict.get(doc_id, (0.0, None))[0]\n",
        "                fused_scores[doc_id] = self.alpha * s_vec + (1 - self.alpha) * s_bm25\n",
        "\n",
        "        fused_results = []\n",
        "        for doc_id, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "            node = None\n",
        "            for r in dense_results + sparse_results:\n",
        "                if r.node.node_id == doc_id:\n",
        "                    node = r.node\n",
        "                    break\n",
        "            fused_results.append(NodeWithScore(node=node, score=score))\n",
        "        return fused_results\n",
        "\n",
        "hybrid_retriever = HybridRetriever(dense_retriever, sparse_retriever, mode=\"rrf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o4DAlsD0QXv"
      },
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# Suppress Warnings and Errors (Optional)\n",
        "# ====================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# For transformers, suppress specific warnings if needed\n",
        "from transformers import logging as hf_logging\n",
        "hf_logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwPLLzGX0QVm"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# Prompt 1 ‚Äî retrieve relevant context\n",
        "prompt_retrieve = PromptTemplate(\n",
        "    \"·π¢e √†w√°r√≠ √†l√†y√© t√≥ y·∫π j√πl·ªç n√≠pa ak·ªçÃÅl√© y√¨√≠: {topic}\\n\"\n",
        ")\n",
        "\n",
        "# Prompt 2 ‚Äî synthesize Yoruba answer\n",
        "prompt_answer = PromptTemplate(\n",
        "    \"\"\"F√∫n un n√≠ √¨d√°h√πn n√≠ √®d√® Yor√πb√° t√≥ d√° l√≥r√≠ √†l√†y√© y√¨√≠.\n",
        "J·ªçÃÄw·ªçÃÅ lo √®d√® t√≥ m·ªçÃÅ, ·π£√†l√†y√© d√°ad√°a, k√≠ o s√¨ t·ªçÃÅka s√≠ or√≠sun n√°√†.\n",
        "\n",
        "√Äl√†y√©:\n",
        "{context}\n",
        "\n",
        "√åd√°h√πn:\n",
        "\"\"\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Mke-Uc0QQr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "import cohere\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 1. HuggingFace Model Loader\n",
        "# =============================\n",
        "def load_huggingface_llm(model_name: str):\n",
        "    \"\"\"\n",
        "    Load a HuggingFace model using 4-bit quantization with BitsAndBytes.\n",
        "    Automatically detects if model is seq2seq or causal.\n",
        "    Falls back to standard precision if quantization fails.\n",
        "    \"\"\"\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        AutoModelForCausalLM,\n",
        "        AutoModelForSeq2SeqLM,\n",
        "        BitsAndBytesConfig,\n",
        "    )\n",
        "    import torch\n",
        "\n",
        "    print(f\"üîÑ Loading {model_name}...\")\n",
        "\n",
        "    # ‚úÖ Define quantization configuration (stable for CUDA 12+)\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",              # recommended quantization type\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,  # use bfloat16 for CUDA 12+\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    # Detect model architecture\n",
        "    if any(x in model_name.lower() for x in [\"t5\", \"mbart\", \"aya\", \"afrolm\", \"bart\"]):\n",
        "        model_loader = AutoModelForSeq2SeqLM\n",
        "    else:\n",
        "        model_loader = AutoModelForCausalLM\n",
        "\n",
        "    try:\n",
        "        # Attempt 4-bit quantized loading\n",
        "        model = model_loader.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quant_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully loaded {model_name} in 4-bit quantized mode.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] 4-bit quantization failed for {model_name}: {e}\")\n",
        "        print(\"‚û°Ô∏è Falling back to full precision mode.\")\n",
        "        model = model_loader.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else \"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "    # Wrap model in HuggingFaceLLM\n",
        "    llm = HuggingFaceLLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "# =============================\n",
        "# 2. Gemini Model Loader\n",
        "# =============================\n",
        "def load_gemini_llm(api_key: str, model: str = \"models/gemini-2.5-flash\"):\n",
        "    \"\"\"Load Google Gemini model.\"\"\"\n",
        "    print(f\"üîÑ Loading Gemini: {model}...\")\n",
        "    llm = Gemini(model=model, api_key=api_key)\n",
        "    print(f\"‚úÖ Gemini loaded successfully!\")\n",
        "    return llm\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 3. Cohere Setup\n",
        "# =============================\n",
        "def setup_cohere_client(api_key: str):\n",
        "    \"\"\"Initialize Cohere client.\"\"\"\n",
        "    print(\"üîÑ Setting up Cohere client...\")\n",
        "    co = cohere.ClientV2(api_key=api_key)\n",
        "    print(\"‚úÖ Cohere client ready!\")\n",
        "    return co\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 3.5 Document Cleaning Functions\n",
        "# =============================\n",
        "def clean_retrieved_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and normalize retrieved text before passing to generation.\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove special characters that might confuse the model\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
        "\n",
        "    # Remove URLs (optional - comment out if URLs are important)\n",
        "    # text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "    # Remove email addresses (optional)\n",
        "    # text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove excessive punctuation\n",
        "    text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
        "\n",
        "    # Trim whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def filter_and_clean_nodes(nodes, min_length: int = 50, max_length: int = 2000, score_threshold: float = 0.0):\n",
        "    \"\"\"\n",
        "    Filter and clean retrieved nodes based on quality criteria.\n",
        "\n",
        "    Args:\n",
        "        nodes: Retrieved nodes from the retriever\n",
        "        min_length: Minimum character length for a node to be kept\n",
        "        max_length: Maximum character length (truncate if longer)\n",
        "        score_threshold: Minimum relevance score (if available)\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with cleaned text and score\n",
        "    \"\"\"\n",
        "    cleaned_nodes = []\n",
        "\n",
        "    for node in nodes:\n",
        "        # Get the text content\n",
        "        text = node.text if hasattr(node, 'text') else str(node)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_retrieved_text(text)\n",
        "\n",
        "        # Skip if too short after cleaning\n",
        "        if len(cleaned_text) < min_length:\n",
        "            continue\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(cleaned_text) > max_length:\n",
        "            cleaned_text = cleaned_text[:max_length] + \"...\"\n",
        "\n",
        "        # Check relevance score if available\n",
        "        score = getattr(node, 'score', None)\n",
        "        if score is not None and score < score_threshold:\n",
        "            continue\n",
        "\n",
        "        # Store cleaned text and score in a dict\n",
        "        cleaned_nodes.append({'text': cleaned_text, 'score': score})\n",
        "\n",
        "    return cleaned_nodes\n",
        "\n",
        "\n",
        "def deduplicate_contexts(nodes):\n",
        "    \"\"\"\n",
        "    Remove duplicate or highly similar contexts.\n",
        "    Accepts list of dicts with 'text' key.\n",
        "    \"\"\"\n",
        "    seen_texts = set()\n",
        "    unique_nodes = []\n",
        "\n",
        "    for node in nodes:\n",
        "        text = node['text']\n",
        "\n",
        "        # Simple deduplication based on first 100 characters\n",
        "        text_signature = text[:100].lower().strip()\n",
        "\n",
        "        if text_signature not in seen_texts:\n",
        "            seen_texts.add(text_signature)\n",
        "            unique_nodes.append(node)\n",
        "\n",
        "    return unique_nodes\n",
        "\n",
        "\n",
        "def yoruba_rag_query_llamaindex(topic: str, top_k: int = 3, clean_context: bool = True):\n",
        "    \"\"\"\n",
        "    RAG query using LlamaIndex LLM (HuggingFace or Gemini via Settings.llm).\n",
        "    Now includes document cleaning before generation.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    retrieved_nodes = dense_retriever.retrieve(topic)[:top_k]\n",
        "    print(f\"üìö Retrieved {len(retrieved_nodes)} documents\")\n",
        "\n",
        "    # Clean and filter retrieved documents\n",
        "    if clean_context:\n",
        "        print(\"üßπ Cleaning retrieved documents...\")\n",
        "        retrieved_nodes = filter_and_clean_nodes(\n",
        "            retrieved_nodes,\n",
        "            min_length=50,\n",
        "            max_length=2000,\n",
        "            score_threshold=0.0\n",
        "        )\n",
        "        retrieved_nodes = deduplicate_contexts(retrieved_nodes)\n",
        "        print(f\"‚ú® After cleaning: {len(retrieved_nodes)} documents\")\n",
        "\n",
        "    # Concatenate contexts\n",
        "    context = \"\\n\\n\".join([n['text'] for n in retrieved_nodes])\n",
        "    print(f\"üìù Context length: {len(context)} characters\")\n",
        "\n",
        "    # Apply Yoruba synthesis prompt\n",
        "    full_prompt = prompt_answer.format(context=context)\n",
        "    print(\"--- Prompt sent to LLM ---\")\n",
        "    print(full_prompt[:500] + \"...\" if len(full_prompt) > 500 else full_prompt)\n",
        "\n",
        "    # Generation configuration for better quality responses\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.7,             # Controls creativity\n",
        "        \"top_p\": 0.9,                   # Nucleus sampling\n",
        "        \"top_k\": 40,                    # Limits token search space\n",
        "        \"repetition_penalty\": 1.15,     # Penalize repeating n-grams\n",
        "        \"max_new_tokens\": 256,          # Limit response length\n",
        "        \"no_repeat_ngram_size\": 3,      # Avoid loops\n",
        "        \"do_sample\": True               # Enables random sampling\n",
        "    }\n",
        "\n",
        "    # Call LLM using Settings.llm with generation config\n",
        "    # Detect Gemini model by class name\n",
        "    llm_class_name = type(Settings.llm).__name__.lower()\n",
        "    if \"gemini\" in llm_class_name:\n",
        "        response = Settings.llm.complete(full_prompt)\n",
        "    else:\n",
        "        response = Settings.llm.complete(full_prompt, **generation_config)\n",
        "\n",
        "\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def yoruba_rag_query_cohere(topic: str, cohere_client, top_k: int = 5, clean_context: bool = True):\n",
        "    \"\"\"\n",
        "    RAG query using Cohere API.\n",
        "    Now includes document cleaning before generation.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    retrieved_nodes = dense_retriever.retrieve(topic)[:top_k]\n",
        "    print(f\"üìö Retrieved {len(retrieved_nodes)} documents\")\n",
        "\n",
        "    # Clean and filter retrieved documents\n",
        "    if clean_context:\n",
        "        print(\"üßπ Cleaning retrieved documents...\")\n",
        "        retrieved_nodes = filter_and_clean_nodes(\n",
        "            retrieved_nodes,\n",
        "            min_length=50,\n",
        "            max_length=2000,\n",
        "            score_threshold=0.0\n",
        "        )\n",
        "        retrieved_nodes = deduplicate_contexts(retrieved_nodes)\n",
        "        print(f\"‚ú® After cleaning: {len(retrieved_nodes)} documents\")\n",
        "\n",
        "    # Concatenate contexts\n",
        "    context = \"\\n\\n\".join([n['text'] for n in retrieved_nodes])\n",
        "\n",
        "    # Call Cohere Chat API\n",
        "    response = cohere_client.chat(\n",
        "        model=\"command-a-03-2025\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"\n",
        "                √åb√©√®r√®: {topic}\n",
        "\n",
        "                √åt√†n √†k·ªçÃÅk·ªçÃÅ (context):\n",
        "                {context}\n",
        "\n",
        "                √åd√°h√πn n√≠ √®d√® Yor√πb√°:\n",
        "                \"\"\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.message.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zcvXA-T0QM1"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Load your embedding model\n",
        "similarity_model=SentenceTransformer(\"BAAI/bge-m3\")\n",
        "from bert_score import score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJlFzHIF0QJn"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from bert_score import score\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Initialize models\n",
        "similarity_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "rouge = Rouge()\n",
        "\n",
        "def evaluate_generation_metrics(\n",
        "    question: str,\n",
        "    contexts: Union[str, List[str]],\n",
        "    answer: str,\n",
        "    reference: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate RAG output quality using BLEU, ROUGE-L, BERTScore, and cosine similarity.\n",
        "    \"\"\"\n",
        "    # Normalize inputs\n",
        "    if isinstance(contexts, str):\n",
        "        contexts = [contexts]\n",
        "    full_context = ' '.join(contexts)\n",
        "\n",
        "    # --- BLEU ---\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_score = sentence_bleu(\n",
        "        [reference.split()],\n",
        "        answer.split(),\n",
        "        smoothing_function=smoothie\n",
        "    )\n",
        "\n",
        "    # --- ROUGE-L ---\n",
        "    rouge_scores = rouge.get_scores(answer, reference, avg=True)\n",
        "    rouge_l = rouge_scores['rouge-l']['f']\n",
        "\n",
        "    # --- BERTScore ---\n",
        "    P, R, F1 = score(\n",
        "        [answer],\n",
        "        [reference],\n",
        "        lang='yo',\n",
        "        model_type='xlm-roberta-large'\n",
        "    )\n",
        "    bert_f1 = F1.mean().item()\n",
        "\n",
        "    # --- Cosine Similarity (semantic similarity) ---\n",
        "    ans_emb = similarity_model.encode(answer, convert_to_tensor=True)\n",
        "    ref_emb = similarity_model.encode(reference, convert_to_tensor=True)\n",
        "    cosine_sim = util.cos_sim(ans_emb, ref_emb).item()\n",
        "\n",
        "    # --- Composite Generation Score ---\n",
        "    composite_gen = round(\n",
        "        (bleu_score + rouge_l + bert_f1 + cosine_sim) / 4, 3\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'BLEU': round(bleu_score, 3),\n",
        "        'ROUGE-L': round(rouge_l, 3),\n",
        "        'BERTScore_F1': round(bert_f1, 3),\n",
        "        'Cosine_Similarity': round(cosine_sim, 3),\n",
        "        'Composite_Gen': composite_gen   # ‚úÖ renamed for pipeline consistency\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_yoruba_rag(question: str, contexts: Union[str, List[str]], answer: str, reference: str):\n",
        "    \"\"\"\n",
        "    Combined RAG evaluation: context-level and generation-level.\n",
        "    \"\"\"\n",
        "    # Context relevance\n",
        "    CR = evaluate_context_relevance(question, contexts)\n",
        "    # Faithfulness\n",
        "    FG = evaluate_faithfulness(answer, contexts)\n",
        "    # Generation-level metrics\n",
        "    gen_scores = evaluate_generation_metrics(question, contexts, answer, reference)\n",
        "\n",
        "    # Composite\n",
        "    composite = (CR + FG + gen_scores['Composite_Generation_Score']) / 3\n",
        "\n",
        "    return {\n",
        "        'Context_Relevance': round(CR, 3),\n",
        "        'Faithfulness': round(FG, 3),\n",
        "        **gen_scores,\n",
        "        'Composite_RAG_Score': round(composite, 3)\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Supporting functions reused from earlier ---\n",
        "def evaluate_context_relevance(arg1: Union[str, List[str]], arg2: Union[str, List[str]]):\n",
        "    if isinstance(arg1, str) and not isinstance(arg2, str):\n",
        "        question = arg1\n",
        "        contexts = arg2\n",
        "    elif isinstance(arg2, str) and not isinstance(arg1, str):\n",
        "        question = arg2\n",
        "        contexts = arg1\n",
        "    else:\n",
        "        question = arg1\n",
        "        contexts = arg2\n",
        "\n",
        "    if isinstance(contexts, str):\n",
        "        contexts = [contexts]\n",
        "\n",
        "    if len(contexts) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    question_emb = similarity_model.encode(question, convert_to_tensor=True)\n",
        "    doc_embs = similarity_model.encode(contexts, convert_to_tensor=True)\n",
        "    sims = util.cos_sim(question_emb, doc_embs)\n",
        "    return float(sims.mean().item())\n",
        "\n",
        "\n",
        "def evaluate_faithfulness(answer: str, context: Union[str, List[str]]):\n",
        "    if isinstance(context, str):\n",
        "        context = [context]\n",
        "    ctx_emb = similarity_model.encode(' '.join(context), convert_to_tensor=True)\n",
        "    ans_emb = similarity_model.encode(answer, convert_to_tensor=True)\n",
        "    cosine_sim = util.cos_sim(ctx_emb, ans_emb).item()\n",
        "    return float(cosine_sim)\n",
        "\n",
        "def evaluate_answer_relevance(question: str, answer: str):\n",
        "    # bert-score expects lists of references & candidates\n",
        "    P, R, F1 = score([answer], [question], lang='yo', model_type='xlm-roberta-large')\n",
        "    return float(F1.mean().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NPbTSr40QF6"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "def cluster_sample(df, domain_col=\"domain\", n_per_domain=5):\n",
        "    \"\"\"\n",
        "    Cluster-based sampling: use embeddings of questions to select 10 diverse samples per domain.\n",
        "    \"\"\"\n",
        "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    sampled = []\n",
        "\n",
        "    for domain, group in df.groupby(domain_col):\n",
        "        group = group.dropna(subset=[\"question\"])\n",
        "        if len(group) <= n_per_domain:\n",
        "            sampled.append(group)\n",
        "            continue\n",
        "\n",
        "        embeddings = embedder.encode(group[\"question\"].tolist())\n",
        "        kmeans = KMeans(n_clusters=n_per_domain, random_state=42)\n",
        "        clusters = kmeans.fit_predict(embeddings)\n",
        "        group[\"cluster\"] = clusters\n",
        "\n",
        "        cluster_sampled = (\n",
        "            group.groupby(\"cluster\")\n",
        "            .apply(lambda x: x.sample(1, random_state=42))\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "        sampled.append(cluster_sampled)\n",
        "\n",
        "    return pd.concat(sampled, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEN40cix0QCf"
      },
      "outputs": [],
      "source": [
        "# Suppose you already have your context documents (from ground_truth.csv)\n",
        "df = pd.read_csv(\"/content/ground_truth.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV3ItRa10P_S"
      },
      "outputs": [],
      "source": [
        " sampled_df = cluster_sample(df, n_per_domain=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUG9npNQ0P4X"
      },
      "outputs": [],
      "source": [
        " # ====================================\n",
        "# ‚ö° Optimized Yoruba RAG Quantitative Evaluation Across 7 Models\n",
        "# ====================================\n",
        "import gc\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from functools import lru_cache\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# üßπ Utility Functions\n",
        "# ====================================\n",
        "def clean_text(text, max_len=512):\n",
        "    \"\"\"Normalize Yoruba text and truncate to avoid long context embedding.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    return text[:max_len]\n",
        "\n",
        "\n",
        "def free_memory():\n",
        "    \"\"\"Force garbage collection and clear GPU memory.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def get_free_gpu_memory():\n",
        "    \"\"\"Return free GPU memory (in GB) for device 0.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return 0\n",
        "    gpu_stats = torch.cuda.mem_get_info()\n",
        "    free_mem_gb = gpu_stats[0] / 1024**3\n",
        "    return round(free_mem_gb, 2)\n",
        "\n",
        "# ====================================\n",
        "# ‚öôÔ∏è Model Setup (Cached)\n",
        "# ====================================\n",
        "@lru_cache(maxsize=8)\n",
        "def set_model(model_name):\n",
        "    \"\"\"Load or cache model/LLM client.\"\"\"\n",
        "    from llama_index.core import Settings\n",
        "    free_mem = get_free_gpu_memory()\n",
        "    print(f\"üß† Detected free GPU memory: {free_mem} GB\")\n",
        "\n",
        "    if model_name == \"gemini\":\n",
        "        llm = load_gemini_llm(api_key=\"*******************************\")\n",
        "        Settings.llm = llm\n",
        "        return llm\n",
        "\n",
        "    elif model_name == \"cohere\":\n",
        "        global cohere_client\n",
        "        cohere_client = setup_cohere_client(api_key=\"*******************************\")\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        llm = load_huggingface_llm(model_name)\n",
        "        Settings.llm = llm\n",
        "        return llm\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# üìä Evaluation Configuration\n",
        "# ====================================\n",
        "model_names = [\n",
        "    \"gemini\" , # Gemini API\n",
        "    \"cohere\",  # Cohere API\n",
        "\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    #\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "   # \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    #\"bigscience/bloomz-7b1\",\n",
        "    #\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your HuggingFace access token here\n",
        "login(\"hf_token_api_key\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OmPqryL0PzR"
      },
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# üöÄ Evaluation Pipeline\n",
        "# ====================================\n",
        "all_results = []\n",
        "start_all = time.time()\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"\\nüß† Evaluating model: {model_name}\")\n",
        "    model_start = time.time()\n",
        "\n",
        "    # Load model efficiently\n",
        "    llm = set_model(model_name)\n",
        "    free_memory()\n",
        "\n",
        "    for _, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=f\"{model_name}\"):\n",
        "        domain = row[\"domain\"]\n",
        "        question = clean_text(row[\"question\"])\n",
        "        reference = clean_text(row[\"reference_answer\"])\n",
        "        context_doc = clean_text(row[\"context_document\"])\n",
        "\n",
        "        try:\n",
        "            # üîπ Generate answer\n",
        "            if model_name == \"cohere\":\n",
        "                answer = yoruba_rag_query_cohere(question, cohere_client, top_k=3)\n",
        "            else:\n",
        "                answer = yoruba_rag_query_llamaindex(question, top_k=3)\n",
        "\n",
        "            # üîπ Metric computation\n",
        "            context_score = evaluate_context_relevance(question, [context_doc])\n",
        "            faith_score = evaluate_faithfulness(answer, [context_doc])\n",
        "            relevance_score = evaluate_answer_relevance(question, answer)\n",
        "            gen_metrics = evaluate_generation_metrics(question=question,\n",
        "                contexts=[context_doc],\n",
        "                answer=answer,\n",
        "                reference=reference)\n",
        "\n",
        "            composite_rag = round(\n",
        "                (context_score + faith_score + relevance_score + gen_metrics[\"Composite_Gen\"]) / 4, 3\n",
        "            )\n",
        "            composite_gen = gen_metrics.get(\"Composite_Gen\", 0)\n",
        "            composite_rag = round((context_score + faith_score + relevance_score + composite_gen) / 4, 3)\n",
        "            print(relevance_score)\n",
        "            print(context_score)\n",
        "            print(relevance_score)\n",
        "            print(gen_metrics)\n",
        "\n",
        "            all_results.append({\n",
        "                \"model\": model_name,\n",
        "                \"domain\": domain,\n",
        "                \"question\": question,\n",
        "                \"reference\": reference,\n",
        "                \"generated_answer\": answer,\n",
        "                \"context_relevance\": context_score,\n",
        "                \"faithfulness\": faith_score,\n",
        "                \"answer_relevance\": relevance_score,\n",
        "                **gen_metrics,\n",
        "                \"Composite_RAG_Score\": composite_rag\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Skipped item: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ Completed {model_name} in {round(time.time() - model_start, 2)} sec\")\n",
        "    free_memory()\n",
        "\n",
        "# ====================================\n",
        "# üßæ Aggregate Results\n",
        "# ====================================\n",
        "df_all = pd.DataFrame(all_results)\n",
        "\n",
        "if df_all.empty:\n",
        "    print(\"\\n‚ö†Ô∏è No evaluation results were recorded ‚Äî check generation or metric functions.\")\n",
        "else:\n",
        "    print(\"\\nüìä Sample of Evaluation Results:\")\n",
        "    print(df_all.head())\n",
        "\n",
        "    # Model-wise summary (average metrics)\n",
        "    summary = (\n",
        "        df_all.groupby(\"model\")[[\"context_relevance\", \"faithfulness\", \"Composite_RAG_Score\"]]\n",
        "        .mean()\n",
        "        .sort_values(\"Composite_RAG_Score\", ascending=False)\n",
        "    )\n",
        "\n",
        "    print(\"\\nüìà Yoruba RAG Evaluation Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Total Evaluation Time: {round(time.time() - start_all, 2)} sec\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T23:07:51.171860Z",
     "iopub.status.busy": "2025-11-04T23:07:51.171365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!pip install cohere --quiet\n",
    "!pip install -U llama-index llama-index-llms-cohere --quiet\n",
    "!pip install llama-index-llms-gemini --quiet\n",
    "!pip install bert-score\n",
    "!pip install -U weaviate-client --quiet\n",
    "!pip install -U llama-index --quiet\n",
    "%pip install -U llama-index-embeddings-huggingface --quiet\n",
    "!pip install -U llama-index-vector-stores-weaviate --quiet\n",
    "!pip install -U bitsandbytes --quiet\n",
    "!pip install -U llama-index-llms-huggingface --quiet\n",
    "%pip install llama-index-retrievers-bm25 --quiet\n",
    "!pip install streamlit --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "import torch\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from sklearn.preprocessing import normalize\n",
    "# Embedding model2Ô∏è‚É£ AfriBERTa embedding (HuggingFace style)\n",
    "# ============================\n",
    "class AfriBERTaEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model_name=\"castorini/afriberta_base\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModel.from_pretrained(model_name).to(self._device)\n",
    "        self._model.eval()\n",
    "\n",
    "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def _embed(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self._tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self._device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "            embeddings = self._mean_pooling(outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
    "            embeddings = normalize(embeddings.cpu().numpy(), norm=\"l2\")\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Implement required abstract methods\n",
    "    # -----------------------------\n",
    "    def _get_text_embedding(self, text: str):\n",
    "        return self._embed(text)[0]\n",
    "\n",
    "    def _get_query_embedding(self, query: str):\n",
    "        return self._embed(query)[0]\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str):\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str):\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "Settings.embed_model = AfriBERTaEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WEAVIATE_URL\"] = \"https://eu3nsymbtcib2x8fy120yw.c0.europe-west3.gcp.weaviate.cloud\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"***********\"\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Best practice: store your credentials in environment variables\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "# Connect to Weaviate Cloud\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#3. Wrap your Weaviate collection\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client,\n",
    "    index_name=\"YorubaChunk\",   # use your actual collection name\n",
    ")\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# 6Ô∏è‚É£ Create index with Qdrant\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from collections import defaultdict\n",
    "\n",
    "# Dense Retriever (AfriBERTa)\n",
    "dense_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# Sparse Retriever (BM25)\n",
    "yoruba_collection = client.collections.get(\"YorubaChunk\")\n",
    "all_docs = [Document(text=obj.properties[\"text\"]) for obj in yoruba_collection.iterator()]\n",
    "sparse_retriever = BM25Retriever(all_docs, similarity_top_k=5)\n",
    "\n",
    "# Hybrid Retriever\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, dense_retriever, sparse_retriever, mode=\"rrf\", alpha=0.5, k=60):\n",
    "        self.dense_retriever = dense_retriever\n",
    "        self.sparse_retriever = sparse_retriever\n",
    "        self.mode = mode\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        dense_results = self.dense_retriever.retrieve(query, **kwargs)\n",
    "        sparse_results = self.sparse_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        dense_dict = {r.node.node_id: (r.score, i + 1) for i, r in enumerate(dense_results)}\n",
    "        sparse_dict = {r.node.node_id: (r.score, i + 1) for i, r in enumerate(sparse_results)}\n",
    "\n",
    "        all_doc_ids = set(dense_dict.keys()) | set(sparse_dict.keys())\n",
    "        fused_scores = defaultdict(float)\n",
    "\n",
    "        if self.mode == \"rrf\":\n",
    "            for doc_id in all_doc_ids:\n",
    "                if doc_id in dense_dict:\n",
    "                    _, rank = dense_dict[doc_id]\n",
    "                    fused_scores[doc_id] += 1.0 / (self.k + rank)\n",
    "                if doc_id in sparse_dict:\n",
    "                    _, rank = sparse_dict[doc_id]\n",
    "                    fused_scores[doc_id] += 1.0 / (self.k + rank)\n",
    "        elif self.mode == \"rsf\":\n",
    "            for doc_id in all_doc_ids:\n",
    "                s_vec = dense_dict.get(doc_id, (0.0, None))[0]\n",
    "                s_bm25 = sparse_dict.get(doc_id, (0.0, None))[0]\n",
    "                fused_scores[doc_id] = self.alpha * s_vec + (1 - self.alpha) * s_bm25\n",
    "\n",
    "        fused_results = []\n",
    "        for doc_id, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            node = None\n",
    "            for r in dense_results + sparse_results:\n",
    "                if r.node.node_id == doc_id:\n",
    "                    node = r.node\n",
    "                    break\n",
    "            fused_results.append(NodeWithScore(node=node, score=score))\n",
    "        return fused_results\n",
    "\n",
    "hybrid_retriever = HybridRetriever(dense_retriever, sparse_retriever, mode=\"rrf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from llama_index.core import Settings\n",
    "import torch\n",
    "import cohere\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1. HuggingFace Model Loader\n",
    "# =============================\n",
    "def load_huggingface_llm(model_name: str):\n",
    "    \"\"\"\n",
    "    Load a HuggingFace model using 4-bit quantization with BitsAndBytes.\n",
    "    Automatically detects if model is seq2seq or causal.\n",
    "    Falls back to standard precision if quantization fails.\n",
    "    \"\"\"\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        AutoModelForSeq2SeqLM,\n",
    "        BitsAndBytesConfig,\n",
    "    )\n",
    "    import torch\n",
    "\n",
    "    print(f\"üîÑ Loading {model_name}...\")\n",
    "\n",
    "    # ‚úÖ Define quantization configuration (stable for CUDA 12+)\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",              # recommended quantization type\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,  # use bfloat16 for CUDA 12+\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Detect model architecture\n",
    "    if any(x in model_name.lower() for x in [\"t5\", \"mbart\", \"aya\", \"afrolm\", \"bart\"]):\n",
    "        model_loader = AutoModelForSeq2SeqLM\n",
    "    else:\n",
    "        model_loader = AutoModelForCausalLM\n",
    "\n",
    "    try:\n",
    "        # Attempt 4-bit quantized loading\n",
    "        model = model_loader.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quant_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"‚úÖ Successfully loaded {model_name} in 4-bit quantized mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] 4-bit quantization failed for {model_name}: {e}\")\n",
    "        print(\"‚û°Ô∏è Falling back to full precision mode.\")\n",
    "        model = model_loader.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else \"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    # Wrap model in HuggingFaceLLM\n",
    "    llm = HuggingFaceLLM(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "# =============================\n",
    "# 2. Gemini Model Loader\n",
    "# =============================\n",
    "def load_gemini_llm(api_key: str, model: str = \"models/gemini-2.5-flash\"):\n",
    "    \"\"\"Load Google Gemini model.\"\"\"\n",
    "    print(f\"üîÑ Loading Gemini: {model}...\")\n",
    "    llm = Gemini(model=model, api_key=api_key)\n",
    "    print(f\"‚úÖ Gemini loaded successfully!\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3. Cohere Setup\n",
    "# =============================\n",
    "def setup_cohere_client(api_key: str):\n",
    "    \"\"\"Initialize Cohere client.\"\"\"\n",
    "    print(\"üîÑ Setting up Cohere client...\")\n",
    "    co = cohere.ClientV2(api_key=api_key)\n",
    "    print(\"‚úÖ Cohere client ready!\")\n",
    "    return co\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3.5 Document Cleaning Functions\n",
    "# =============================\n",
    "def clean_retrieved_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize retrieved text before passing to generation.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters that might confuse the model\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # Remove URLs (optional - comment out if URLs are important)\n",
    "    # text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses (optional)\n",
    "    # text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove excessive punctuation\n",
    "    text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
    "    \n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_and_clean_nodes(nodes, min_length: int = 50, max_length: int = 2000, score_threshold: float = 0.0):\n",
    "    \"\"\"\n",
    "    Filter and clean retrieved nodes based on quality criteria.\n",
    "    \n",
    "    Args:\n",
    "        nodes: Retrieved nodes from the retriever\n",
    "        min_length: Minimum character length for a node to be kept\n",
    "        max_length: Maximum character length (truncate if longer)\n",
    "        score_threshold: Minimum relevance score (if available)\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with cleaned text and score\n",
    "    \"\"\"\n",
    "    cleaned_nodes = []\n",
    "    \n",
    "    for node in nodes:\n",
    "        # Get the text content\n",
    "        text = node.text if hasattr(node, 'text') else str(node)\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned_text = clean_retrieved_text(text)\n",
    "        \n",
    "        # Skip if too short after cleaning\n",
    "        if len(cleaned_text) < min_length:\n",
    "            continue\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(cleaned_text) > max_length:\n",
    "            cleaned_text = cleaned_text[:max_length] + \"...\"\n",
    "        \n",
    "        # Check relevance score if available\n",
    "        score = getattr(node, 'score', None)\n",
    "        if score is not None and score < score_threshold:\n",
    "            continue\n",
    "        \n",
    "        # Store cleaned text and score in a dict\n",
    "        cleaned_nodes.append({'text': cleaned_text, 'score': score})\n",
    "    \n",
    "    return cleaned_nodes\n",
    "\n",
    "\n",
    "def deduplicate_contexts(nodes):\n",
    "    \"\"\"\n",
    "    Remove duplicate or highly similar contexts.\n",
    "    Accepts list of dicts with 'text' key.\n",
    "    \"\"\"\n",
    "    seen_texts = set()\n",
    "    unique_nodes = []\n",
    "    \n",
    "    for node in nodes:\n",
    "        text = node['text']\n",
    "        \n",
    "        # Simple deduplication based on first 100 characters\n",
    "        text_signature = text[:100].lower().strip()\n",
    "        \n",
    "        if text_signature not in seen_texts:\n",
    "            seen_texts.add(text_signature)\n",
    "            unique_nodes.append(node)\n",
    "    \n",
    "    return unique_nodes\n",
    "\n",
    "\n",
    "def yoruba_rag_query_llamaindex(topic: str, top_k: int = 3, clean_context: bool = True):\n",
    "    \"\"\"\n",
    "    RAG query using LlamaIndex LLM (HuggingFace or Gemini via Settings.llm).\n",
    "    Now includes document cleaning before generation.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_nodes = dense_retriever.retrieve(topic)[:top_k]\n",
    "    print(f\"üìö Retrieved {len(retrieved_nodes)} documents\")\n",
    "    \n",
    "    # Clean and filter retrieved documents\n",
    "    if clean_context:\n",
    "        print(\"üßπ Cleaning retrieved documents...\")\n",
    "        retrieved_nodes = filter_and_clean_nodes(\n",
    "            retrieved_nodes, \n",
    "            min_length=50,\n",
    "            max_length=2000,\n",
    "            score_threshold=0.0\n",
    "        )\n",
    "        retrieved_nodes = deduplicate_contexts(retrieved_nodes)\n",
    "        print(f\"‚ú® After cleaning: {len(retrieved_nodes)} documents\")\n",
    "    \n",
    "    # Concatenate contexts\n",
    "    context = \"\\n\\n\".join([n['text'] for n in retrieved_nodes])\n",
    "    print(f\"üìù Context length: {len(context)} characters\")\n",
    "    \n",
    "    # Apply Yoruba synthesis prompt\n",
    "    full_prompt = prompt_answer.format(context=context)\n",
    "    print(\"--- Prompt sent to LLM ---\")\n",
    "    print(full_prompt[:500] + \"...\" if len(full_prompt) > 500 else full_prompt)\n",
    "    \n",
    "    # Generation configuration for better quality responses\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,             # Controls creativity\n",
    "        \"top_p\": 0.9,                   # Nucleus sampling\n",
    "        \"top_k\": 40,                    # Limits token search space\n",
    "        \"repetition_penalty\": 1.15,     # Penalize repeating n-grams\n",
    "        \"max_new_tokens\": 256,          # Limit response length\n",
    "        \"no_repeat_ngram_size\": 3,      # Avoid loops\n",
    "        \"do_sample\": True               # Enables random sampling\n",
    "    }\n",
    "    \n",
    "    # Call LLM using Settings.llm with generation config\n",
    "    # Detect Gemini model by class name\n",
    "    llm_class_name = type(Settings.llm).__name__.lower()\n",
    "    if \"gemini\" in llm_class_name:\n",
    "        response = Settings.llm.complete(full_prompt)\n",
    "    else:\n",
    "        response = Settings.llm.complete(full_prompt, **generation_config)\n",
    "    \n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "def yoruba_rag_query_cohere(topic: str, cohere_client, top_k: int = 5, clean_context: bool = True):\n",
    "    \"\"\"\n",
    "    RAG query using Cohere API.\n",
    "    Now includes document cleaning before generation.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_nodes = hybrid_retriever.retrieve(topic)[:top_k]\n",
    "    print(f\"üìö Retrieved {len(retrieved_nodes)} documents\")\n",
    "    \n",
    "    # Clean and filter retrieved documents\n",
    "    if clean_context:\n",
    "        print(\"üßπ Cleaning retrieved documents...\")\n",
    "        retrieved_nodes = filter_and_clean_nodes(\n",
    "            retrieved_nodes, \n",
    "            min_length=50,\n",
    "            max_length=2000,\n",
    "            score_threshold=0.0\n",
    "        )\n",
    "        retrieved_nodes = deduplicate_contexts(retrieved_nodes)\n",
    "        print(f\"‚ú® After cleaning: {len(retrieved_nodes)} documents\")\n",
    "    \n",
    "    # Concatenate contexts\n",
    "    context = \"\\n\\n\".join([n['text'] for n in retrieved_nodes])\n",
    "    \n",
    "    # Call Cohere Chat API\n",
    "    response = cohere_client.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                √åb√©√®r√®: {topic}\n",
    "\n",
    "                √åt√†n √†k·ªçÃÅk·ªçÃÅ (context):\n",
    "                {context}\n",
    "\n",
    "                √åd√°h√πn n√≠ √®d√® Yor√πb√°:\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load your embedding model\n",
    "similarity_model=SentenceTransformer(\"BAAI/bge-m3\")\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Initialize models\n",
    "similarity_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "rouge = Rouge()\n",
    "\n",
    "def evaluate_generation_metrics(\n",
    "    question: str, \n",
    "    contexts: Union[str, List[str]], \n",
    "    answer: str, \n",
    "    reference: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate RAG output quality using BLEU, ROUGE-L, BERTScore, and cosine similarity.\n",
    "    \"\"\"\n",
    "    # Normalize inputs\n",
    "    if isinstance(contexts, str):\n",
    "        contexts = [contexts]\n",
    "    full_context = ' '.join(contexts)\n",
    "    \n",
    "    # --- BLEU ---\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu(\n",
    "        [reference.split()], \n",
    "        answer.split(), \n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "\n",
    "    # --- ROUGE-L ---\n",
    "    rouge_scores = rouge.get_scores(answer, reference, avg=True)\n",
    "    rouge_l = rouge_scores['rouge-l']['f']\n",
    "\n",
    "    # --- BERTScore ---\n",
    "    P, R, F1 = score(\n",
    "        [answer], \n",
    "        [reference], \n",
    "        lang='yo', \n",
    "        model_type='xlm-roberta-large'\n",
    "    )\n",
    "    bert_f1 = F1.mean().item()\n",
    "\n",
    "    # --- Cosine Similarity (semantic similarity) ---\n",
    "    ans_emb = similarity_model.encode(answer, convert_to_tensor=True)\n",
    "    ref_emb = similarity_model.encode(reference, convert_to_tensor=True)\n",
    "    cosine_sim = util.cos_sim(ans_emb, ref_emb).item()\n",
    "\n",
    "    # --- Composite Generation Score ---\n",
    "    composite_gen = round(\n",
    "        (bleu_score + rouge_l + bert_f1 + cosine_sim) / 4, 3\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'BLEU': round(bleu_score, 3),\n",
    "        'ROUGE-L': round(rouge_l, 3),\n",
    "        'BERTScore_F1': round(bert_f1, 3),\n",
    "        'Cosine_Similarity': round(cosine_sim, 3),\n",
    "        'Composite_Gen': composite_gen   # ‚úÖ renamed for pipeline consistency\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_yoruba_rag(question: str, contexts: Union[str, List[str]], answer: str, reference: str):\n",
    "    \"\"\"\n",
    "    Combined RAG evaluation: context-level and generation-level.\n",
    "    \"\"\"\n",
    "    # Context relevance\n",
    "    CR = evaluate_context_relevance(question, contexts)\n",
    "    # Faithfulness\n",
    "    FG = evaluate_faithfulness(answer, contexts)\n",
    "    # Generation-level metrics\n",
    "    gen_scores = evaluate_generation_metrics(question, contexts, answer, reference)\n",
    "\n",
    "    # Composite\n",
    "    composite = (CR + FG + gen_scores['Composite_Generation_Score']) / 3\n",
    "\n",
    "    return {\n",
    "        'Context_Relevance': round(CR, 3),\n",
    "        'Faithfulness': round(FG, 3),\n",
    "        **gen_scores,\n",
    "        'Composite_RAG_Score': round(composite, 3)\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Supporting functions reused from earlier ---\n",
    "def evaluate_context_relevance(arg1: Union[str, List[str]], arg2: Union[str, List[str]]):\n",
    "    if isinstance(arg1, str) and not isinstance(arg2, str):\n",
    "        question = arg1\n",
    "        contexts = arg2\n",
    "    elif isinstance(arg2, str) and not isinstance(arg1, str):\n",
    "        question = arg2\n",
    "        contexts = arg1\n",
    "    else:\n",
    "        question = arg1\n",
    "        contexts = arg2\n",
    "\n",
    "    if isinstance(contexts, str):\n",
    "        contexts = [contexts]\n",
    "\n",
    "    if len(contexts) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    question_emb = similarity_model.encode(question, convert_to_tensor=True)\n",
    "    doc_embs = similarity_model.encode(contexts, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(question_emb, doc_embs)\n",
    "    return float(sims.mean().item())\n",
    "\n",
    "\n",
    "def evaluate_faithfulness(answer: str, context: Union[str, List[str]]):\n",
    "    if isinstance(context, str):\n",
    "        context = [context]\n",
    "    ctx_emb = similarity_model.encode(' '.join(context), convert_to_tensor=True)\n",
    "    ans_emb = similarity_model.encode(answer, convert_to_tensor=True)\n",
    "    cosine_sim = util.cos_sim(ctx_emb, ans_emb).item()\n",
    "    return float(cosine_sim)\n",
    "    \n",
    "def evaluate_answer_relevance(question: str, answer: str):\n",
    "    # bert-score expects lists of references & candidates\n",
    "    P, R, F1 = score([answer], [question], lang='yo', model_type='xlm-roberta-large')\n",
    "    return float(F1.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def cluster_sample(df, domain_col=\"domain\", n_per_domain=5):\n",
    "    \"\"\"\n",
    "    Cluster-based sampling: use embeddings of questions to select 10 diverse samples per domain.\n",
    "    \"\"\"\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    sampled = []\n",
    "\n",
    "    for domain, group in df.groupby(domain_col):\n",
    "        group = group.dropna(subset=[\"question\"])\n",
    "        if len(group) <= n_per_domain:\n",
    "            sampled.append(group)\n",
    "            continue\n",
    "\n",
    "        embeddings = embedder.encode(group[\"question\"].tolist())\n",
    "        kmeans = KMeans(n_clusters=n_per_domain, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        group[\"cluster\"] = clusters\n",
    "\n",
    "        cluster_sampled = (\n",
    "            group.groupby(\"cluster\")\n",
    "            .apply(lambda x: x.sample(1, random_state=42))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        sampled.append(cluster_sampled)\n",
    "\n",
    "    return pd.concat(sampled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you already have your context documents (from ground_truth.csv)\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/kaggle/input/ground/ground_truth.csv\")\n",
    "sampled_df = cluster_sample(df, n_per_domain=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # ====================================\n",
    "# ‚ö° Optimized Yoruba RAG Quantitative Evaluation Across 7 Models\n",
    "# ====================================\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# üßπ Utility Functions\n",
    "# ====================================\n",
    "def clean_text(text, max_len=512):\n",
    "    \"\"\"Normalize Yoruba text and truncate to avoid long context embedding.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text[:max_len]\n",
    "\n",
    "\n",
    "def free_memory():\n",
    "    \"\"\"Force garbage collection and clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def get_free_gpu_memory():\n",
    "    \"\"\"Return free GPU memory (in GB) for device 0.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0\n",
    "    gpu_stats = torch.cuda.mem_get_info()\n",
    "    free_mem_gb = gpu_stats[0] / 1024**3\n",
    "    return round(free_mem_gb, 2)\n",
    "    \n",
    "# ====================================\n",
    "# ‚öôÔ∏è Model Setup (Cached)\n",
    "# ====================================\n",
    "@lru_cache(maxsize=8)\n",
    "def set_model(model_name):\n",
    "    \"\"\"Load or cache model/LLM client.\"\"\"\n",
    "    from llama_index.core import Settings\n",
    "    free_mem = get_free_gpu_memory()\n",
    "    print(f\"üß† Detected free GPU memory: {free_mem} GB\")\n",
    "\n",
    "    if model_name == \"gemini\":\n",
    "        llm = load_gemini_llm(api_key=\"\")\n",
    "        Settings.llm = llm\n",
    "        return llm\n",
    "\n",
    "    elif model_name == \"cohere\":\n",
    "        global cohere_client\n",
    "        cohere_client = setup_cohere_client(api_key=\"\")\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        llm = load_huggingface_llm(model_name)\n",
    "        Settings.llm = llm\n",
    "        return llm\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# üìä Evaluation Configuration\n",
    "# ====================================\n",
    "model_names = [\n",
    "    #\"gemini\" , # Gemini API\n",
    "    #\"cohere\",  # Cohere API\n",
    "#\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    #\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    #\"bigscience/bloomz-7b1\",\n",
    "    #\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your HuggingFace access token here\n",
    "login(\"hf_token_api_key\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ===============================\n",
    "# Base Yoruba RAG class\n",
    "# ===============================\n",
    "class YorubaRAGBase:\n",
    "    \"\"\"Base class for Yoruba RAG pipelines (for extension).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.llm = Settings.llm\n",
    "        if self.llm is None:\n",
    "            raise RuntimeError(\"‚ùå No LLM found in Settings. Please initialize one before using YorubaRAGBase.\")\n",
    "        self.index = self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"Return the global VectorStoreIndex (already built externally).\"\"\"\n",
    "        global index\n",
    "        return index\n",
    "# ============================================================\n",
    "# HYBRID RETRIEVER (Dense + Sparse)\n",
    "# ============================================================\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, dense_retriever, sparse_retriever, mode=\"rrf\", alpha=0.5, k=60):\n",
    "        self.dense_retriever = dense_retriever\n",
    "        self.sparse_retriever = sparse_retriever\n",
    "        self.mode = mode\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        dense_results = self.dense_retriever.retrieve(query, **kwargs)\n",
    "        sparse_results = self.sparse_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        dense_dict = {r.node.node_id: (r.score, i + 1) for i, r in enumerate(dense_results)}\n",
    "        sparse_dict = {r.node.node_id: (r.score, i + 1) for i, r in enumerate(sparse_results)}\n",
    "\n",
    "        all_doc_ids = set(dense_dict.keys()) | set(sparse_dict.keys())\n",
    "        fused_scores = defaultdict(float)\n",
    "\n",
    "        # Reciprocal Rank Fusion (RRF)\n",
    "        if self.mode == \"rrf\":\n",
    "            for doc_id in all_doc_ids:\n",
    "                if doc_id in dense_dict:\n",
    "                    _, rank = dense_dict[doc_id]\n",
    "                    fused_scores[doc_id] += 1.0 / (self.k + rank)\n",
    "                if doc_id in sparse_dict:\n",
    "                    _, rank = sparse_dict[doc_id]\n",
    "                    fused_scores[doc_id] += 1.0 / (self.k + rank)\n",
    "\n",
    "        # Rank Score Fusion (RSF)\n",
    "        elif self.mode == \"rsf\":\n",
    "            for doc_id in all_doc_ids:\n",
    "                s_vec = dense_dict.get(doc_id, (0.0, None))[0]\n",
    "                s_bm25 = sparse_dict.get(doc_id, (0.0, None))[0]\n",
    "                fused_scores[doc_id] = self.alpha * s_vec + (1 - self.alpha) * s_bm25\n",
    "\n",
    "        fused_results = []\n",
    "        for doc_id, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            node = None\n",
    "            for r in dense_results + sparse_results:\n",
    "                if r.node.node_id == doc_id:\n",
    "                    node = r.node\n",
    "                    break\n",
    "            if node:\n",
    "                fused_results.append(NodeWithScore(node=node, score=score))\n",
    "        return fused_results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ADVANCED RAG (Yoruba Query Expansion + Filtering + Rerank)\n",
    "# ============================================================\n",
    "class AdvancedRAG(YorubaRAGBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_retriever,\n",
    "        sparse_retriever,\n",
    "        llm=None,\n",
    "        hybrid_mode=\"rrf\",\n",
    "        hybrid_alpha=0.5,\n",
    "        hybrid_k=60,\n",
    "        rerank_top_k=3,\n",
    "        use_semantic_rerank=True,\n",
    "        use_contextual_filter=True,\n",
    "        embed_model=Settings.embed_model,\n",
    "        verbose=True,\n",
    "        diversity_threshold=0.9,\n",
    "        min_text_length=50,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.retriever = HybridRetriever(\n",
    "            dense_retriever, sparse_retriever,\n",
    "            mode=hybrid_mode, alpha=hybrid_alpha, k=hybrid_k\n",
    "        )\n",
    "        self.hybrid_alpha = hybrid_alpha\n",
    "        self.rerank_top_k = rerank_top_k\n",
    "        self.use_semantic_rerank = use_semantic_rerank\n",
    "        self.embed_model = embed_model\n",
    "        self.use_contextual_filter = use_contextual_filter\n",
    "        self.verbose = verbose\n",
    "        self.diversity_threshold = diversity_threshold\n",
    "        self.min_text_length = min_text_length\n",
    "\n",
    "    # --------------------------\n",
    "    # Yoruba Query Expansion\n",
    "    # --------------------------\n",
    "    def yoruba_query_expansion(self, original_query: str) -> List[str]:\n",
    "        expansion_rules = {\n",
    "            \"K√≠ ni\": [\"√åtum·ªçÃÄ\", \"√åd√°h√πn\", \"√Äl√†y√©\"],\n",
    "            \"·π¢√©\": [\"«∏j·∫πÃÅ\", \"B√°wo ni\", \"·π¢√© o l√®\"],\n",
    "            \"√¨t√†n\": [\"√†r√≤s·ªç\", \"√†l·ªçÃÅ\", \"√¨r√≠r√≠\"],\n",
    "            \"B√°wo\": [\"B√≠ o ·π£e\", \"·ªåÃÄn√†\"],\n",
    "        }\n",
    "        expanded = [original_query]\n",
    "        for term, expansions in expansion_rules.items():\n",
    "            if term in original_query:\n",
    "                expanded.extend([original_query.replace(term, e) for e in expansions])\n",
    "        return list(dict.fromkeys(expanded))  # deduplicate\n",
    "\n",
    "    # --------------------------\n",
    "    # Semantic Re-ranking\n",
    "    # --------------------------\n",
    "    def semantic_rerank(self, query: str, nodes: List, top_k: Optional[int] = None) -> List:\n",
    "        if not nodes:\n",
    "            return []\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.rerank_top_k\n",
    "\n",
    "        query_emb = np.array(self.embed_model.get_query_embedding(query))\n",
    "        scored_nodes = []\n",
    "        for node in nodes:\n",
    "            text = getattr(node, \"text\", str(node))\n",
    "            text_emb = np.array(self.embed_model.get_text_embedding(text[:512]))\n",
    "            sim = np.dot(query_emb, text_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(text_emb))\n",
    "            scored_nodes.append((node, sim))\n",
    "\n",
    "        scored_nodes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            top_scores = [f\"{s:.3f}\" for _, s in scored_nodes[:3]]\n",
    "            print(f\"üîÑ Reranked {len(nodes)} nodes | Top scores: {top_scores}\")\n",
    "\n",
    "        return [n for n, _ in scored_nodes[:top_k]]\n",
    "\n",
    "    # --------------------------\n",
    "    # Contextual Filtering\n",
    "    # --------------------------\n",
    "    def contextual_filter(self, nodes: List, domain: Optional[str] = None) -> List:\n",
    "        if not nodes:\n",
    "            return []\n",
    "\n",
    "        filtered = []\n",
    "        seen_texts = []\n",
    "\n",
    "        for node in nodes:\n",
    "            text = getattr(node, \"text\", str(node))\n",
    "            if len(text) < self.min_text_length:\n",
    "                continue\n",
    "\n",
    "            diverse = True\n",
    "            if self.diversity_threshold < 1.0:\n",
    "                text_vec = np.array(self.embed_model.get_text_embedding(text[:512]))\n",
    "                for seen in seen_texts:\n",
    "                    seen_vec = np.array(self.embed_model.get_text_embedding(seen[:512]))\n",
    "                    sim = np.dot(text_vec, seen_vec) / (np.linalg.norm(text_vec) * np.linalg.norm(seen_vec))\n",
    "                    if sim > self.diversity_threshold:\n",
    "                        diverse = False\n",
    "                        break\n",
    "\n",
    "            if not diverse:\n",
    "                continue\n",
    "\n",
    "            filtered.append(node)\n",
    "            seen_texts.append(text)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"üéØ Filtered: {len(filtered)} / {len(nodes)} nodes remain\")\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    # --------------------------\n",
    "    # Main Query Function\n",
    "    # --------------------------\n",
    "    def query(self, question: str, domain: Optional[str] = None) -> Dict[str, Any]:\n",
    "        start = time.time()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(f\"üîç Query: {question}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "        # Step 1: Query Expansion\n",
    "        expanded_queries = self.yoruba_query_expansion(question)\n",
    "\n",
    "        # Step 2: Hybrid Retrieval\n",
    "        all_nodes = []\n",
    "        for q in expanded_queries[:3]:\n",
    "            try:\n",
    "                nodes = self.retriever.retrieve(q)\n",
    "                all_nodes.extend(nodes)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Retrieval failed for '{q}': {e}\")\n",
    "\n",
    "        # Step 3: Deduplicate\n",
    "        unique_nodes = list({getattr(n, \"node_id\", str(n)): n for n in all_nodes}.values())\n",
    "\n",
    "        # Step 4: Context Filter\n",
    "        if self.use_contextual_filter:\n",
    "            unique_nodes = self.contextual_filter(unique_nodes, domain)\n",
    "\n",
    "        # Step 5: Rerank\n",
    "        if self.use_semantic_rerank:\n",
    "            unique_nodes = self.semantic_rerank(question, unique_nodes)\n",
    "\n",
    "         # Step 6: Cohere Path (override normal synthesis)\n",
    "        if hasattr(self.llm, \"cohere_client\"):\n",
    "            print(\"ü§ñ Using Cohere for Yoruba RAG generation...\")\n",
    "            response_text = yoruba_rag_query_cohere(\n",
    "                topic=question,\n",
    "                cohere_client=self.llm.cohere_client,\n",
    "                dense_retriever=self.dense_retriever,\n",
    "                top_k=5,\n",
    "                clean_context=True\n",
    "            )\n",
    "            elapsed = round(time.time() - start, 2)\n",
    "            return {\n",
    "                \"answer\": response_text,\n",
    "                \"source_nodes\": unique_nodes,\n",
    "                \"response_time\": elapsed,\n",
    "                \"variant\": \"modular_cohere\",\n",
    "                \"retrieval\": f\"hybrid_Œ±={self.hybrid_alpha}\",\n",
    "                \"expanded_queries\": expanded_queries,\n",
    "            }\n",
    "\n",
    "        # Step 7: Generate Answer\n",
    "        if not unique_nodes:\n",
    "            return {\"answer\": \"No relevant documents found.\", \"source_nodes\": [], \"response_time\": time.time() - start}\n",
    "\n",
    "        synthesizer = get_response_synthesizer(llm=self.llm)\n",
    "        response = synthesizer.synthesize(question, unique_nodes)\n",
    "        elapsed = round(time.time() - start, 2)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"‚úÖ Response ready in {elapsed}s | {len(unique_nodes)} nodes used\")\n",
    "\n",
    "        return {\n",
    "            \"answer\": str(response),\n",
    "            \"source_nodes\": unique_nodes,\n",
    "            \"response_time\": elapsed,\n",
    "            \"variant\": \"advanced\",\n",
    "            \"retrieval\": f\"hybrid_Œ±={self.hybrid_alpha}\",\n",
    "            \"expanded_queries\": expanded_queries,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "advanced_prompt = \"\"\"\n",
    "# ADVANCED YOR√ôB√Å KNOWLEDGE RETRIEVAL AND GENERATION\n",
    "\n",
    "## CONTEXT DOCUMENT (√ÄK·ªåÃÅL·∫∏ÃÅ √åS√ÄL·∫∏ÃÄ √åM·ªåÃÄ)\n",
    "W√†√° lo √†k·ªçs√≠l·∫πÃÄ y√¨√≠ n√¨kan l√°ti d√°h√πn √¨b√©√®r√® t√≥ t·∫πÃÄl√© e. M√° ·π£e lo √¨m·ªçÃÄ t√≥ k·ªçj√° √®y√≠ t√≥ w√† n√≠n√∫ √†k·ªçs√≠l·∫πÃÄ y√¨√≠.\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "## USER'S QUERY (√åB√â√àR√à OL√ôL√í)\n",
    "D√°h√πn √¨b√©√®r√® t√≠ √≥ w√† n√≠s√†l·∫πÃÄ y√¨√≠ d√°rad√°ra, l√°ti in√∫ √†k·ªçs√≠l·∫πÃÄ t√≥ w√† l√≥k√®.\n",
    "---\n",
    "{query}\n",
    "---\n",
    "\n",
    "## YOR√ôB√Å OUTPUT REQUIREMENTS (√ÄW·ªåN √íFIN √åD√ÅH√ôN YOR√ôB√Å)\n",
    "1. **√àd√®:** Gbogbo √¨d√°h√πn gb·ªçÃÅd·ªçÃÄ w√† n√≠ **√àd√® Yor√πb√°** p·∫πÃÄl√∫ **√†m√¨-oh√πn** t√≠ √≥ t·ªçÃÅ (·ªç, ·π£, ·∫π, √†, √®, √¨, √≤, √π).\n",
    "2. **√åd√°h√πn:** √åd√°h√πn gb·ªçÃÅd·ªçÃÄ j·∫πÃÅ **k√≠k√∫n √†ti w√≠w·ªçÃÅk·∫πÃÅr·∫πÃÅk·∫πÃÅr·∫πÃÅ**, n√≠ f√≠f·∫π·π£·∫πÃÄ r·∫π n√≠n√∫ √†k·ªçs√≠l·∫πÃÄ (context).\n",
    "3. **√ågb√†w·ªçÃÄn:** √åd√°h√πn gb·ªçÃÅd·ªçÃÄ j·∫πÃÅ **3 s√≠ 5 gb√≥l√≥h√πn** n√≠ g√≠g√πn.\n",
    "4. **√ågb√©kal·∫πÃÄ:** B·∫πÃÄr·∫πÃÄ √¨d√°h√πn r·∫π n√≠ t√†√†r√†t√†.\n",
    "\n",
    "## ANSWER (√åD√ÅH√ôN)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "def advanced_query(rag, question: str):\n",
    "    \"\"\"Test Yor√πb√° answer generation with advanced prompt.\"\"\"\n",
    "    # Retrieve top contexts\n",
    "    results = rag.query(question)\n",
    "    nodes = results[\"source_nodes\"]\n",
    "\n",
    "    # Join context text\n",
    "    context_text = \"\\n\\n\".join([n.text for n in nodes])\n",
    "\n",
    "    # Apply prompt template\n",
    "    prompt = PromptTemplate(advanced_prompt)\n",
    "    formatted = prompt.format(context=context_text, query=question)\n",
    "\n",
    "    # Generate answer\n",
    "    try:\n",
    "        synthesizer = get_response_synthesizer(llm=rag.llm)\n",
    "        response = synthesizer.synthesize(formatted, nodes)\n",
    "        print(\"‚úÖ Yor√πb√° Answer:\\n\", str(response))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating Yor√πb√° answer: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# üöÄ Evaluation Pipeline\n",
    "# ====================================\n",
    "all_results = []\n",
    "start_all = time.time()\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nüß† Evaluating model: {model_name}\")\n",
    "    model_start = time.time()\n",
    "\n",
    "    # Load model efficiently\n",
    "    llm = set_model(model_name)\n",
    "    free_memory()\n",
    "    adv_rag =  AdvancedRAG(dense_retriever=dense_retriever,sparse_retriever=sparse_retriever,embed_model=Settings.embed_model,verbose=True,llm=llm)\n",
    "\n",
    "    for _, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=f\"{model_name}\"):\n",
    "        domain = row[\"domain\"]\n",
    "        question = clean_text(row[\"question\"])\n",
    "        reference = clean_text(row[\"reference_answer\"])\n",
    "        context_doc = clean_text(row[\"context_document\"])\n",
    "\n",
    "        try:\n",
    "            # üîπ Generate answer\n",
    "            response = adv_rag.query(question=question)\n",
    "            answer = response[\"answer\"]\n",
    "\n",
    "            # üîπ Metric computation\n",
    "            context_score = evaluate_context_relevance(question, [context_doc])\n",
    "            faith_score = evaluate_faithfulness(answer, [context_doc])\n",
    "            relevance_score = evaluate_answer_relevance(question, answer)\n",
    "            gen_metrics = evaluate_generation_metrics(question=question,\n",
    "                contexts=[context_doc],\n",
    "                answer=answer,\n",
    "                reference=reference)\n",
    "\n",
    "            composite_rag = round(\n",
    "                (context_score + faith_score + relevance_score + gen_metrics[\"Composite_Gen\"]) / 4, 3\n",
    "            )\n",
    "            composite_gen = gen_metrics.get(\"Composite_Gen\", 0)\n",
    "            composite_rag = round((context_score + faith_score + relevance_score + composite_gen) / 4, 3)\n",
    "            print(relevance_score)\n",
    "            print(context_score)\n",
    "            print(relevance_score)\n",
    "            print(gen_metrics)\n",
    "\n",
    "            all_results.append({\n",
    "                \"model\": model_name,\n",
    "                \"domain\": domain,\n",
    "                \"question\": question,\n",
    "                \"reference\": reference,\n",
    "                \"generated_answer\": answer,\n",
    "                \"context_relevance\": context_score,\n",
    "                \"faithfulness\": faith_score,\n",
    "                \"answer_relevance\": relevance_score,\n",
    "                **gen_metrics,\n",
    "                \"Composite_RAG_Score\": composite_rag\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipped item: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ Completed {model_name} in {round(time.time() - model_start, 2)} sec\")\n",
    "    free_memory()\n",
    "\n",
    "# ====================================\n",
    "# üßæ Aggregate Results\n",
    "# ====================================\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "if df_all.empty:\n",
    "    print(\"\\n‚ö†Ô∏è No evaluation results were recorded ‚Äî check generation or metric functions.\")\n",
    "else:\n",
    "    print(\"\\nüìä Sample of Evaluation Results:\")\n",
    "    print(df_all.head())\n",
    "\n",
    "    # Model-wise summary (average metrics)\n",
    "    summary = (\n",
    "        df_all.groupby(\"model\")[[\"context_relevance\", \"faithfulness\", \"Composite_RAG_Score\"]]\n",
    "        .mean()\n",
    "        .sort_values(\"Composite_RAG_Score\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìà Yoruba RAG Evaluation Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Total Evaluation Time: {round(time.time() - start_all, 2)} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8628426,
     "sourceId": 13581196,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
